{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Two: Data Preparation\n",
    "\n",
    "Expected time to complete: 45 minutes\n",
    "\n",
    "## Goal of this Course\n",
    "\n",
    "In this course we will transform our dataset into a format that can be used with a statistical model. We will first explain some key concepts that will help us to understand in which scope our problem stands.  \n",
    "\n",
    "The course is split into the following parts:\n",
    "- <a href='#context'>Context</a> \n",
    "- <a href='#supervised_vs_unsupervised'>Supervised vs Unsupervised Learning</a>\n",
    "- <a href='#classif_vs_reg'>Classification vs Regression</a>\n",
    "- <a href='#model_output'>Model Output</a> \n",
    "    - <a href='#load_data'>Load the Data</a>\n",
    "    - <a href='#define_target'>Define our Target</a>\n",
    "    - <a href='#basic_statistics'>Descriptive Statistics</a>\n",
    "\n",
    "\n",
    "- <a href='#model_input'>Model Input</a> \n",
    "    - <a href='#feature_selection'>Feature Selection</a>\n",
    "    - <a href='#basic_statistics2'>Descriptive Statistics</a>\n",
    "\n",
    "\n",
    "- <a href='#pair_input_output'>Pair Model Input with Model Output</a> \n",
    "- <a href='#missing_data'>Check for Missing Data</a> \n",
    "\n",
    "<a id='context'></a>\n",
    "# Context\n",
    "\n",
    "In the previous tutorial we formulated a data problem, chose a dataset and performed some initial steps that cleaned  and then helped us explore the structure of our data. In this second course we will start looking at how to _prepare_ our dataset to be compatible with the kind of statistical models that can help us solve our problem.\n",
    "\n",
    "Data preparation or _data wrangling_ is a crucial step in the modelling process. It refers to how we create a dataset to be ingested into at least one of the many statistical models at our disposal. \n",
    "\n",
    "We need to make sure we understand the scope of the problem:\n",
    "- Do we have a _target_ variable? In other words, will our model predict something (the target) or will it look for unlabeled structure? (supervised vs unsupervised models)\n",
    "- If predicting, what form will the prediction target have? Is it represented as discrete categories or a linear scale (classification or regression)?\n",
    "- If predicting, what is the granularity? User level? User x time level?\n",
    "- What features of our dataset are we going to use as input to our model (feature selection)?\n",
    "\n",
    "Before we dive back into the code, let's explain a few core concepts that will be helpful to understand before we go any further.\n",
    "\n",
    "<a id='supervised_vs_unsupervised'></a>\n",
    "# Supervised vs Unsupervised Learning\n",
    "\n",
    "An important disctinction to make when preparing your data for analysis is whether your problem is suitable for “supervised” or “unsupervised” learning models. \n",
    " \n",
    "__Unsupervised learning__ models receive input (_explanatory/predictor/independent_ variables or _features_) but no output (or _dependent/target_ variable)  and are a way of discovering latent structure in a set of data (clustering is an example of unsupervised learning). Unsupervised models are very useful when working with unlabelled datasets. These models can then be (and often are) combined with supervised models.\n",
    " \n",
    "__Supervised learning__ models learn a mathematical function between an input (_explanatory/predictor/independent_ variables) and an output (the _dependent/target_ variable). These models are used in situations where you know what you want to predict and have explicit input-output pairs for your model to be trained upon.\n",
    "\n",
    "In our current project we want to forecast the minutes watched on iPlayer (output or _target_ variable) based on past behavior (input or _explanatory/predictor/independent_ variables). We have input-output pairs and are, therefore, in the supervised learning framework.\n",
    "\n",
    "<a id='classif_vs_reg'></a>\n",
    "# Classification vs Regression\n",
    "\n",
    "Within the supervised learning class of problems there is a further split, based upon the nature of the target variable: discrete or linear. If a dataset is being used to predict one of a finite set of categories, this is called a classification problem and is tackled by a specific set of models called _classifiers_. A simple example of a classification problem is a spam email filter where the text of the email must be classified as either legitimate or not. A further exampleof a _multi-classification_ problem is handwriting recognition, where a hand-written letter is classified as one of a set of 36 (26 letters and 10 digits).\n",
    "\n",
    "Alternatively, some data problems require the prediction of linear or non-categorical variables. This class of problems is approached using a class of model called _regressors_. An example of a regression problem would be to predict the number of hits an article receives by analysing the contents of its headline.\n",
    "\n",
    "In our current project we want to predict if a user watched any content within a two-week period (a classification problem), and we also want to know _how much_ content a user watched within a two-week period (a regression problem). We will therefore be using both classifiers and regressors in this project.\n",
    "\n",
    "<a id='model_output'></a>\n",
    "# Model Output\n",
    "\n",
    "<a id='load_data'></a>\n",
    "## Load the Data\n",
    "Now we read our data from the last course back in and check that it looks reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our output dataset from course 1\n",
    "data = pd.read_csv('iplayer_data_c1.csv')\n",
    "\n",
    "# Check that it all looks how we expect it to.\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should all look very familiar by now. When computing the distribution of our observations among the `twoweek` variable (Course 1 - _Exploratory data analysis_) we saw that we had roughly the same amount of data, except for week 0. Here, as it is not clear why it should have less data (meaning it may be a sign of something not being right), we decide to remove this group from all our subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=data[data['twoweek']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.twoweek.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='define_target'></a>\n",
    "## Define our Target\n",
    "\n",
    "In this section we will walk through the process of how to create the output of our model. This variable that supervised models predict is know as the _target variable_, the _output variable_ or the _dependent variable_.\n",
    "\n",
    "In our project we want to forecast the next two weeks behaviour, so we need to aggregate our target metric for the last two weeks of data available, i.e. how many minutes of content, if any, did each individual audience member consume within `twoweek` 8. \n",
    "\n",
    "Our target variable will be different for our two different types of model:\n",
    "\n",
    "- In our _regression model_ our target is the total minutes watched within the final two week period.\n",
    "\n",
    "- In our _classification model_  our target is a binary variable (0 or 1), where we will forecast whether a user will watch iPlayer or not (total minutes watched > 0) within the final two week period.\n",
    "\n",
    "\n",
    "### Regression Model\n",
    "\n",
    "In our specific problem, we want to forecast what _individual_ users will do. Therefore, we need to pivot our dataset from an _events view_ to a _user view_,  to build both our target and features.\n",
    "\n",
    "To calculate minutes watched for each audience member we create a _pivot table_. This process results in a table where each individual audience member is represented as a row and each column describes the total minutes watched for a given two-week period. As all the audience members who consumed no content during a given two-week period are represented as \"NA\" after this operation, we replace all NAs with zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to compute the total minutes watched within twoweek 8 for each user\n",
    "target_reg=pd.pivot_table(data,\n",
    "                          values='min_watched',\n",
    "                          index=['user_id'],\n",
    "                          columns=['twoweek'],\n",
    "                          aggfunc=sum)[8].reset_index()\n",
    "\n",
    "\n",
    "# We fill the NAs with 0: corresponds to the users without observations for the last twoweek\n",
    "# i.e. who didn't watch anything\n",
    "target_reg=target_reg.fillna(0)\n",
    "target_reg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Model\n",
    "\n",
    "To calcualate our target variable for the classification model we simply replace all numbers from the minutes watched column that are above zero with 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build the dummy variable based on the minutes watched on this twoweek\n",
    "target_class=target_reg.copy()\n",
    "target_class[8]=np.where(target_class[8]>0,1,0)\n",
    "target_class.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='basic_statistics'></a>\n",
    "## Descriptive Statistics\n",
    "\n",
    "Both for a quick consistency check - and to have business insights related to engagement - we will compute some descriptive statistics on our target variables.\n",
    "\n",
    "### Regression Output - minutes watched\n",
    "First, let's have a look at the basic statistics for the minutes watched per user in the final two-week group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics for a quantitative variable\n",
    "target_reg.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that on average the users watched 76mn on `twoweek` 8. The minutes watched aren't equally distributed among the users though as the median is equal to 0 and the standard error quite huge.\n",
    "\n",
    "Let's plot the histogram to have a better idea of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the minutes watched\n",
    "target_reg[8].hist(bins=100)\n",
    "plt.title('Distribution of target variable (total minutes watched in final two-week group)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 100 bins and the scale reaches 10000 therefore each bar represents 100 minutes. This indicates the vast majority of people (represented as the first bar) consumed less than 100 minutes in the final two-week period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Output - did the user watch iPlayer or not\n",
    "\n",
    "Now let's have a look at the counts of the the classifier target variable. As expected (we see just zeros and ones) it appears that there are more viewers that did not consume any content in the final two-week period than did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the dummy variable did the user watch or not\n",
    "target_class[8].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Distribution of target variable (total minutes watched in final two-week group)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio \n",
    "print(\"More precisely, \"+\n",
    "      str(round(sum(target_class[8])/len(target_class)*100,2))+\n",
    "      \"% of the users in our sample watched iPlayer within the last two-week period.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model_input'></a>\n",
    "# Model Input\n",
    "\n",
    "In this section we will walk through the process of how to select and calculate input to our model. The input to a statistical model can often be confusing as it can be described in many different ways. _Independent variables, explanatory variables, predictor variables_ and _features_ are all ways of describing the same thing: the input to your statistical model.\n",
    "\n",
    "<a id='feature_selection'></a>\n",
    "## Feature Selection\n",
    "As mentioned in the first course, in most situations the process of selecting our input or _feature engineering_ is an iterative one until you get the feature set that neither _underfits_ or _overfits_ the data.\n",
    "\n",
    "A feature set that does not contain sufficient information about the output variable will often result in the model underfitting (this can usually be identified by a high _training error_). The solution here is often to add more features. If the feature set contains features that are sensitive to spurious and random elements of the dataset (and not the underlying population it should be an approximation of), _overfitting_ occurs. Overfitting is characterised by low _training error_ and high _test error_. Overfitting can be tackled by reducing the complexity of your model (often removing features) or using regularisation techniques (https://www.quora.com/What-is-regularization-in-machine-learning). A larger and more diverse training set also helps to reduce overfitting. We will come back to the concept of training and test errors in the following course.\n",
    "\n",
    "There are various feature selection tools that can be used together with cross-validation to optimise your feature set (e.g. _stepwise regression_ - https://en.wikipedia.org/wiki/Stepwise_regression).  \n",
    " \n",
    "In our project we choose to focus on features that describe the viewing habits of a particular user (e.g. “average completion”, “most watched genre”, ”time watched“).\n",
    "\n",
    "### User Granularity\n",
    "The input variables for regression and classifications models are no different here, so we will perform the same process for each. Just as for our target variable, our input variables must have a `user_id` granularity (i.e. each must be paired with a unique audience member).\n",
    "\n",
    "N.B. As our dataset contains temporal properties, we could have performed a more granular pivot on `user_id x twoweek` (i.e. a user represented as seven rows - one for each two-week group). Splitting the dataset up in this way, allows certain types of model to exploit any temporal contingencies that may exist in the data. To keep things simple, here we stick to a single row per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a function that pivots the data based on customer\n",
    "# and gives us all the features we need\n",
    "def pivot_data(dataframe):\n",
    "    #How many minutes did each person watch in each 2 week period\n",
    "    data=pd.pivot_table(dataframe,values='min_watched', \n",
    "                        index=['user_id'],columns=['twoweek'], aggfunc=sum)\n",
    "    # Fill the weeks they didn't watch in with 0s\n",
    "    data.fillna(0,inplace=True)\n",
    "    # How much of average did each viewer watch?\n",
    "    data['average_completion']=dataframe.groupby('user_id')['percentage_watched'].mean()\n",
    "    # How many sessions did the person have with us\n",
    "    data['total_sessions']=dataframe.groupby('user_id')['streaming_id'].nunique()\n",
    "    # How many times has the viewer watched something\n",
    "    data['number_watched']=dataframe.groupby('user_id')['streaming_id'].count()\n",
    "    # Genre most watched by the viewer\n",
    "    data['most_genre']=pd.pivot_table(dataframe,values='min_watched', index=['user_id'],\n",
    "                                      columns=['enriched_genre'], aggfunc=sum).idxmax(axis=1)\n",
    "    # Number of genres watched\n",
    "    data['num_genre']=pd.pivot_table(dataframe,values='min_watched', index=['user_id'],\n",
    "                                     columns=['enriched_genre'], aggfunc=sum).count(axis=1)\n",
    "    # Favourite day of the week to watch\n",
    "    data['most_weekday']=pd.pivot_table(dataframe,values='min_watched', index=['user_id'],\n",
    "                                        columns=['weekday'], aggfunc=sum).idxmax(axis=1)\n",
    "    # Number of weekdays watched\n",
    "    data['num_weekday']=pd.pivot_table(dataframe,values='min_watched', index=['user_id'],\n",
    "                                       columns=['weekday'], aggfunc=sum).count(axis=1)\n",
    "    # Favorite time of day to watch\n",
    "    data['most_timeday']=pd.pivot_table(dataframe,values='min_watched', index=['user_id'],\n",
    "                                        columns=['time_of_day'], aggfunc=sum).idxmax(axis=1)\n",
    "    # Number of times of day\n",
    "    data['num_timeday']=pd.pivot_table(dataframe,values='min_watched', index=['user_id'],\n",
    "                                       columns=['time_of_day'], aggfunc=sum).count(axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to consider here only the \"past data\", i.e. get rid of the last twoweek obs\n",
    "# which corresponds to our target\n",
    "features=pivot_data(data[data['twoweek']<8])\n",
    "features.reset_index().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a set of features for each user. It currently includes:\n",
    "- the minutes watched on a two-week basis for the past 14 weeks `1`, `2`,..., `7`\n",
    "\n",
    "And aggregated over this 14 weeks timeframe:\n",
    "- `average_completion` - the average completion when watching a piece of content \n",
    "- `total_sessions` - the number of sessions \n",
    "- `number_watched` - the number of time a user watched something \n",
    "- `most_genre` - the main genre watched in terms of minutes and not number of pieces of content\n",
    "- `num_genre` - the number of different genre watched\n",
    "- `most_weekday` - the favourite day of the week to watch - again in minutes watched\n",
    "- `num_weekday` - the number of differents days of the week a user watched something\n",
    "- `most_timeday` - the favourite time of the day to watch - again in minutes watched\n",
    "- `num_timeday` - the number of differents times of the day a user watched something\n",
    "\n",
    "This set of variables constitute our features space to build the models on. Each row in this table (without the userID) is referred to as a _feature vector_.  Note that we could have imagined lots of other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummification\n",
    "Most models only take in quantitative data; data represented with numbers. We therefore need to convert those that contain strings to categorical numeric representations. This process is often referred to as creating _dummy variables_. To do this we will replace each unique string within a categorical variable with an aditional column which can contain either a zero or a one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn our categorical variables into bins so that we can run models on this\n",
    "features=pd.get_dummies(features).reset_index()\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naming\n",
    "\n",
    "Here we change the name of the columns representing the two-week minutes watched variables to make them generic:  `tw_lag7_watched`, `tw_lag5_watched`, ..., `tw_lag1_watched`.\n",
    "\n",
    "If we want to use our model later for forecasting purposes on a new dataset with a new timeframe, this will make things much less confusing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the column names generic\n",
    "features = features.rename(columns={1:'tw_lag7_watched',\n",
    "                                    2:'tw_lag6_watched',\n",
    "                                    3:'tw_lag5_watched',\n",
    "                                    4:'tw_lag4_watched',\n",
    "                                    5:'tw_lag3_watched',\n",
    "                                    6:'tw_lag2_watched',\n",
    "                                    7:'tw_lag1_watched'})\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='basic_statistics2'></a>\n",
    "## Descriptive Statistics\n",
    "\n",
    "As for the output variables, we can compute some basic statistics for our input variables.\n",
    "\n",
    "### Distributions\n",
    "We will first visualise their distributions by plotting histograms. There are 35 features in all so going into details of all of them would take a lot of time, but you should notice that - broadly speaking - they should reflect the distributions we observed in the previous course.\n",
    "\n",
    "#### Minutes watched per two-week group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram \n",
    "for i in [x for x in list(features.columns) if 'tw_lag' in x]:\n",
    "    features[i].hist(bins=50)\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These distributions underline __different levels of engagement__ within the users on this two-week timeframe.\n",
    "\n",
    "\n",
    "#### Total number of sessions over the entire timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "features['total_sessions'].hist(bins=50)\n",
    "plt.title('Total number of sessions per user')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of unique pieces of content watched over the entire timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "features['number_watched'].hist(bins=50)\n",
    "plt.title('Number of unique pieces of content watched per user')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average completion over the entire timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "features['average_completion'].hist(bins=50)\n",
    "plt.title('Average completion per user')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you were wondering about the different shape of this distribution compared to the _Programme duration_ plot in the _Descriptive Statistics_ section in Course 1 \n",
    "\n",
    "The first distribution (in course 1) is a representation of the individual views; this shows that overwhelmingly people turn off within the first 2 minutes or finish the whole thing (1 or 0).  The second distribution (above) is the average completion of an audience member. So if they registered two views: one completed, and one switched off after 10 seconds, they would register in this second distribution as 0.5 average completion.\n",
    "So the skew towards zero is an indication that people start more shows than they complete.\n",
    "The bumps at 1 and zero represent viewers that either complete nothing they start or complete everything they start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diversity in the consumption of iPlayer\n",
    "\n",
    "We will have a look here at the distribution of the following variables:\n",
    "- `num_genre` - number of different genres watched over the entire timeframe\n",
    "- `num_weekday` - number of different days of the week the user watched iPlayer over the entire timeframe\n",
    "- `num_timeday` - number of different time of the day the user watched iPlayer over the entire timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram \n",
    "for i in ['num_genre', 'num_weekday','num_timeday']:\n",
    "    features[i].value_counts().sort_index().plot(kind='bar')\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: for few users these variables are null. These users only have observations for the last two-week group that we are not considering here. So we have null rows in our feature space. We will get rid of them later in the course. \n",
    "\n",
    "#### Favourite genre over the entire timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's merge the different dummies related to the favourite genre\n",
    "# in order to have only one histogram \n",
    "favourite_genre=dict()\n",
    "for i in [x for x in list(features.columns) if 'most_genre' in x]:\n",
    "    favourite_genre[i.split('most_genre_',1)[1]]=sum(features[i]) \n",
    "pd.Series(favourite_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "pd.Series(favourite_genre).sort_index().plot(kind='bar')\n",
    "plt.title('Favourite genre watched per user')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Favourite day of the week over the entire timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's merge the different dummies related to the favourite day of week\n",
    "# in order to have only one histogram \n",
    "favourite_weekday=dict()\n",
    "for i in [x for x in list(features.columns) if 'most_weekday' in x]:\n",
    "    favourite_weekday[i.split('most_weekday_',1)[1]]=sum(features[i]) \n",
    "pd.Series(favourite_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "pd.Series(favourite_weekday).sort_index().plot(kind='bar')\n",
    "plt.title('Favourite day of the week to watch iPlayer per user')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Favourite time of the day over the entire timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's merge the different dummies related to the favourite time of the day\n",
    "# in order to have only one histogram \n",
    "favourite_timeday=dict()\n",
    "for i in [x for x in list(features.columns) if 'most_timeday' in x]:\n",
    "    favourite_timeday[i.split('most_timeday_',1)[1]]=sum(features[i]) \n",
    "pd.Series(favourite_timeday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "pd.Series(favourite_timeday).sort_index().plot(kind='bar')\n",
    "plt.title('Favourite time of the day to watch iPlayer per user')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we don't learn much more compared to the descriptive exploration of the previous course. Nervertheless, it is a mandatory consistency check and it can sometimes allow to spot outliers at the _user level_.\n",
    "\n",
    "### Visualising the Relationships between Features\n",
    "\n",
    "A _correlation matrix_ is a good way of getting a quick grasp of the relationships between our features. Correlation is a statistical test which measures the strength of a linear relationship between two variables. The correlation coefficient is a value between -1 and 1. Variables that are perfectly correlated or perfectly inversely-correlated  have coefficients of 1 and -1 respectively. Variables that have no linear relationship will have a coeffient of zero. A correlation matrix calculates coefficients for every pair of variable and illustrates them _n_ x _n_ grid (where n= the number of features). \n",
    "\n",
    "For many models (_regressions_ for example) we have to avoid multicolinearity in our features space. If input variables are correlated, some assumptions in the modelling framework won't hold anymore and it will result in inaccurate results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.matshow(features.corr())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder on the different input variables \n",
    "input_var=dict()\n",
    "i=0\n",
    "for x in features.columns:\n",
    "    input_var[i]=x\n",
    "    i+=1\n",
    "input_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the correlation matrix of our input features above,  we can infer that there is a strong linear relationship between `total_sessions` and `number_watched`. We decided to remove `number_watched` for the rest of the analyses as it is highly correlated with other variables. We do this because because some of the models we will be using later in our analyses assume that our features are not dependent upon each other. This would be a violation of multicolinearity in regression models. Some other variables are highly correlated but not to that extent; so we will keep them. \n",
    "\n",
    "Correlation matrix is then useful in the feature selection process. It helps identify _redundancy_ between your features and therefore identify candidates for removal when needing to reduce complexity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features=features.drop(['number_watched'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pair_input_output'></a>\n",
    "# Pair Model Input with Model Output\n",
    "\n",
    "Now we need to ensure that our input variables match up with our output variable. Each row in our input table should match up to the same `user_id` in our output table. If features or target values are missing for a given user we will get rid of this observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the unique users in both the features and the target\n",
    "users_target=target_reg['user_id'].unique()\n",
    "users_features=features['user_id'].unique()\n",
    "\n",
    "# Find those users that are in the target but not in the feature\n",
    "target_not_feature=[]\n",
    "for user in users_target:\n",
    "    if user not in users_features:\n",
    "        target_not_feature.append(user)\n",
    "\n",
    "# Find those users that are in the feature but not in the target\n",
    "feature_not_target=[]\n",
    "for user in users_features:\n",
    "    if user not in users_target:\n",
    "        feature_not_target.append(user)\n",
    "\n",
    "# Print the size of the two sets\n",
    "print('In target but not feature:',len(target_not_feature),\n",
    "      '- In feature but not target:' ,len(feature_not_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: it's actually normal to have noone missing in the second case because we built the target variables based on the entire population of the training data. It's more a consistency check here.\n",
    "\n",
    "We need to remove any users that don't have recorded viewing behaviour before `twoweek` 8. These users can be explained in two ways: either they did not consume any content in the first seven two-week groups or they only signed up to iPlayer in the final two-week group. Neither of these groups are useful from a modeling perspective so we will remove them all. \n",
    "\n",
    "NB: A `time_since_account_created` variable could be an informative feature to consider in a future model. However, we might need some _business logic_ to avoid what is often referred to as _left-censoring_ issues; i.e. as we can't usually observe the entire historic, it's often difficult to have a signing date for all users. We might have began the tracking of our data after the first user accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will set the index to the user_id as this will make it easier to drop rows\n",
    "# Then we drop the rows and then turn the remaining column into an array\n",
    "target_reg=target_reg.set_index(['user_id'])\n",
    "target_reg.drop(target_not_feature,inplace=True)\n",
    "target_reg.reset_index(inplace=True)\n",
    "target_reg=target_reg[8].values\n",
    "\n",
    "# Same for the classification\n",
    "target_class=target_class.set_index(['user_id'])\n",
    "target_class.drop(target_not_feature,inplace=True)\n",
    "target_class.reset_index(inplace=True)\n",
    "target_class=target_class[8].values\n",
    "        \n",
    "# Check to make sure the outcome makes sense\n",
    "print(target_reg[:10])\n",
    "print(target_class[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: We turn our target dataframes into arrays because it's more suitable for the modeling part afterwards. But the data doesn't change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the size of our datasets\n",
    "print('Number of samples in the training feature set:',len(features))\n",
    "print('Number of samples in the training target set (classification):',\n",
    "      len(target_class))\n",
    "print('Number of samples in the training target set (regression):',\n",
    "      len(target_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='missing_data'></a>\n",
    "# Check for Missing Data\n",
    "\n",
    "The last thing we need to do is to replace any possible missing values with zeros and remove the `user_id` field from each row. This field contains useful information for indexing but it does not contain useful modeling information. Therefore, we shall move it from a normal column to an index column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will fill remaining missing values with 0s as we don't know any better\n",
    "features=features.set_index(['user_id'])\n",
    "features.fillna(0,inplace=True)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now that we have our data in the perfect condition to ingest into a statistical model, we shall save it on disk so that we can continue working on it in the next course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features.to_csv('features.csv')\n",
    "np.savetxt('target.txt',(target_reg,target_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this course we looked at the process of preparing our dataset to be ingested into a statistical model.\n",
    "\n",
    "- We learned about how data should be prepared in different ways based upon the kind of problem you have (supervised or unsupervised) and the kind of model you intend to use (classification or regression).\n",
    "- We learned that for supervised learning problems, input/output pairs need to be calculated from our original dataset. \n",
    "- We learned about useful tools such as the generation of pivot tables that helps translate our dataset from event-centered rows to user-centered rows.\n",
    "\n",
    "In the next course we will start the modelling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
