{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course One: Data Exploration\n",
    "\n",
    "Expected time to complete: 45 to 60 minutes\n",
    "\n",
    "## Goal of this Course\n",
    "During this course we will introduce our problem and have a first look at the data. This is one of the most important parts of the modelling process: formulate our problem and explore the data we are going to work with.\n",
    "\n",
    "This course is split into the following parts:\n",
    "- <a href='#context'>Context</a> \n",
    "- <a href='#data_sampling'>Data Sampling</a> \n",
    "- <a href='#import_data'>First Overview of the Data</a>\n",
    "    - <a href='#load_data'>Load the Data</a>\n",
    "    - <a href='#feature_engineering'>Feature Engineering</a>\n",
    "\n",
    "\n",
    "- <a href='#data_analysis'>Exploratory Data Analysis</a>\n",
    "    - <a href='#quality_check'>Data Quality Check</a>\n",
    "    - <a href='#scope'>Scope</a>\n",
    "    - <a href='#basic_statistics'>Descriptive Statistics</a>\n",
    "\n",
    "\n",
    "<a id='context'></a>\n",
    "# Context\n",
    "## Background\n",
    "\n",
    "Cultivating brand loyalty is key for the BBC to remain relevant in a data-driven world dominated by the likes of Youtube, Netflix and Facebook. We know that tens of thousands of viewers consume our content on iPlayer everyday but we still have very little understanding of what makes them come back for more. In order to keep our audience engaged, it is vital that we develop audience statistics that reflect how likely they are to continue consuming our content. These metrics will then enable us to gain greater insights into whether we are surfacing the right content to the right audience members. These activities will be vital for retaining and growing our audiences both domestically and around the world. \n",
    "\n",
    "## Problem Formulation\n",
    "One way of measuring the engagement of a user is to count how much content they have consumed over a given period.  Engagement, measured in this way, may be affected by many different factors ranging from the kind of content they enjoy to the time of day they consume it.\n",
    " \n",
    "In our current exercise, what we want to understand is how an audience member consumes content, based upon their past behaviour. More specifically, what we will do in this case is to investigate if there is a relationship between an audience member’s viewing behaviour (what and when they have watched on iPlayer) over a 14-week period and the amount of content they consumed in the subsequent two-week period.\n",
    "\n",
    "<a id='data_sampling'></a>\n",
    "# Data Sampling \n",
    "\n",
    "## Theory\n",
    "The data you use to solve your problem is called your dataset. To ensure that any inferences you make from your dataset can be generalised to “the real world” it is important to ensure that the data you choose for your dataset is indeed a good representation of the wider population it is sampled from and, in particular, free from systematic biases. Regardless of what machine-learning techniques you eventually use, if your dataset is not a good approximation of the population it is supposed to represent, you will struggle to find a satisfactory solution to your problem.\n",
    "\n",
    "For more information on data sampling please see the Wikipedia page here:\n",
    "https://en.wikipedia.org/wiki/Sampling_(statistics)  \n",
    "\n",
    "\n",
    "## Our Dataset\n",
    "For our particular problem, we have a dataset containing __the viewing history of 10,000 anonymised iPlayer viewers__. To ensure that this is a representative sample those viewers were sampled from all over the UK and are accurate representations over all age groups, socio-economic class, race and gender. \n",
    "\n",
    "Download the dataset used in these exercises from this repository.\n",
    "\n",
    "Unzip the dataset and ensure it is named `iplayer_data_sample_janapr2017.csv` and in the project directory with these training notebooks.\n",
    "\n",
    "<a id='import_data'></a>\n",
    "# First Overview of the Data\n",
    "The code below, and indeed throughout the entire training course, shows one possible way to approach the problem. Other ways are always possible, and comparing them is where the fun starts, so we leave that to you!\n",
    "\n",
    "<a id='install_requirements'></a>\n",
    "## Install requirements\n",
    "Before we start to load and explore our dataset, we need to install some libraries. To make it simple for now, let's install them where you are running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter==1.0.0 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from -r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: scikit-learn==0.19.1 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from -r requirements.txt (line 2)) (0.19.1)\n",
      "Requirement already satisfied: pandas==0.21.0 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from -r requirements.txt (line 3)) (0.21.0)\n",
      "Requirement already satisfied: numpy==1.13.3 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from -r requirements.txt (line 4)) (1.13.3)\n",
      "Requirement already satisfied: scipy==1.0.0 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from -r requirements.txt (line 5)) (1.0.0)\n",
      "Requirement already satisfied: matplotlib==2.1.0 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from -r requirements.txt (line 6)) (2.1.0)\n",
      "Requirement already satisfied: qtconsole in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (4.4.3)\n",
      "Requirement already satisfied: notebook in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (5.7.8)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (5.1.0)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (6.0.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (7.4.2)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from pandas==0.21.0->-r requirements.txt (line 3)) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from pandas==0.21.0->-r requirements.txt (line 3)) (2.8.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from matplotlib==2.1.0->-r requirements.txt (line 6)) (0.10.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from matplotlib==2.1.0->-r requirements.txt (line 6)) (1.12.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from matplotlib==2.1.0->-r requirements.txt (line 6)) (2.4.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: traitlets in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 1)) (4.3.2)\n",
      "Requirement already satisfied: jupyter-client>=4.1 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 1)) (5.2.4)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: pygments in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 1)) (2.3.1)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (18.0.1)\n",
      "Requirement already satisfied: terminado>=0.8.1 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (0.8.2)\n",
      "Requirement already satisfied: nbformat in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: Send2Trash in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (2.10.1)\n",
      "Requirement already satisfied: tornado<7,>=4.1 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: ipython>=5.0.0 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (7.4.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from jupyter-console->jupyter==1.0.0->-r requirements.txt (line 1)) (2.0.9)\n",
      "Requirement already satisfied: widgetsnbextension~=3.4.0 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: mistune>=0.8.1 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.8.4)\n",
      "Requirement already satisfied: bleach in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: testpath in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.4.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (1.4.2)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.5.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.3)\n",
      "Requirement already satisfied: decorator in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from traitlets->qtconsole->jupyter==1.0.0->-r requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: pywinpty>=0.5; os_name == \"nt\" in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from terminado>=0.8.1->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (0.5.5)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from nbformat->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from jinja2->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (1.1.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (41.0.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (0.1.0)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (0.13.3)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (0.7.5)\n",
      "Requirement already satisfied: win-unicode-console>=0.5; sys_platform == \"win32\" and python_version < \"3.6\" in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (0.5)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from prompt-toolkit<2.1.0,>=2.0.0->jupyter-console->jupyter==1.0.0->-r requirements.txt (line 1)) (0.1.7)\n",
      "Requirement already satisfied: webencodings in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from bleach->nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.5.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (0.14.11)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (19.1.0)\n",
      "Requirement already satisfied: parso>=0.3.0 in c:\\users\\gillro~1\\docume~1\\datala~1\\virtua~1\\lib\\site-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (0.4.0)\n"
     ]
    }
   ],
   "source": [
    "# Install all python libraries required for this course.\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later we're going to explain how to do it inside a virtual environment, but let's keep it simple for now.\n",
    "\n",
    "<a id='load_data'></a>\n",
    "## Load the Data\n",
    "After installing the required libraries, it's time to load the data and get a first idea what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all relevent libraries to analyse the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# This line makes sure that our graphs are rendered within the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Set options to display all columns\n",
    "pd.set_option(\"display.max_columns\",None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*pandas* is a very comprehensive library that allows us to manipulate datasets and perform data analysis. Among others it has a great datetime parsing function which will make handling datetimes later much easier!\n",
    "\n",
    "Make sure to have your dataset in the same folder as your notebook. Or specify its path `'path/to/data/iplayer_data_sample_janapr2017.csv'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a pandas dataframe\n",
    "# Display the size of the dataset and first couple of rows to see what it looks like\n",
    "data=pd.read_csv('iplayer_data_sample_janapr2017.csv', parse_dates=['start_date_time'])\n",
    "print(data.shape)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output gives us a broad sense of what our data look likes. We can start creating additional derived columns that we think could explain why and how much people will watch on iPlayer. This step is called __feature engineering__. \n",
    "\n",
    "<a id='feature_engineering'></a>\n",
    "## Feature Engineering\n",
    "Feature engineering is the process of identifying the input variables (_time of the day, content genre_ etc.) that will be used by your model to predict the output variable (_number of minutes watched_). Feature engineering is _very_ important as the performance of your model is directly dependent upon the features you choose as input. Statistical models are essentially just pattern detectors. Different types of models have strengths and weaknesses in specific situations but they can only find structure in your data if the information exists in your features, in the first place.\n",
    "\n",
    "In most situations the process of feature engineering is an iterative one. Finding the best subset of explanatory variables is not straightforward and we will come back to this later. In the meantime, we will enhance the data by calculating some additional features that we think can help predict the viewing-time of iPlayer users. \n",
    "\n",
    "Our hypothesis is that the more you interact with iPlayer, the more likely you are to return. So the additional data columns are aimed at understanding _the breadth of the user's interactions with us_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhance the data with a few additional columns\n",
    "# Here we create some functions that we will apply to our dataset columns afterwards\n",
    "\n",
    "# Get the weekday\n",
    "def get_weekday(formated_time):\n",
    "    return 'weekday_' + str(formated_time.weekday())\n",
    "\n",
    "# Get the two-week number. We will forecast the minutes watched in the last two weeks\n",
    "# Python3 rounds 0.5 up and down depending on the integer so we can't use round function\n",
    "def get_twoweeknumber(formated_time):\n",
    "    return math.floor(formated_time.isocalendar()[1]/2.0)\n",
    "\n",
    "# Get the time of day\n",
    "def get_timeofday(formated_time):\n",
    "    hour=formated_time.hour\n",
    "    if hour in range(5,13):\n",
    "        return 'Morning'\n",
    "    elif hour in range(13,18):\n",
    "        return 'Afternoon'\n",
    "    elif hour in range(18,23):\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "# Parse the programme duration \n",
    "# and turn it into minutes so that we can compute the percentage of time watched\n",
    "# we assume programme_duration is in hh:mm:ss\n",
    "def parse_programme_duration(unformated_time):\n",
    "    try:\n",
    "        timeparts=unformated_time.split(':')\n",
    "        return int(timeparts[0])*60.0+int(timeparts[1])+int(timeparts[2])/60\n",
    "    except:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the functions to the dataframe\n",
    "data['weekday']=data['start_date_time'].apply(get_weekday)\n",
    "data['time_of_day']=data['start_date_time'].apply(get_timeofday)\n",
    "data['programme_duration_mins']=data['programme_duration'].apply(parse_programme_duration)\n",
    "data['twoweek']=data['start_date_time'].apply(get_twoweeknumber)\n",
    "\n",
    "# Convert time viewed and length of programme into minutes\n",
    "data['min_watched']=data['time_viewed']/(60000.0)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_analysis'></a>\n",
    "# Exploratory Data Analysis\n",
    "\n",
    "Once we have our dataset, enhanced with some additional features we can start its exploration. First of all, we need to check the quality of the data we are working with. Most data sources are very \"noisy\"; often data is missing for some observations or data can also be wrong. It is therefore crucial to spend some time cleaning the data to avoid a \"garbage in, garbage out\" problem. Cleaning involves both inferring _missing values_ and identifying _outliers_ (statistically improbable values). When dealing with missing or unusual values, always keep in mind the \"business\" and context of your analysis. \n",
    "\n",
    "Generally the investigation and enrichment of data is quite a cyclical process. As you investigate the data you have new ideas on how to fix and enrich it and you will then again check whether you have any missing or clearly wrong values in your dataset. \n",
    "\n",
    "Exploratory data analysis also allows us to get a _feel_ for our dataset and provides useful insights regarding the best techniques or models to use later on in the modelling stage. Data exploration can involve many different techniques and approaches but will invariably include generating _descriptive statistics_ and _visualising distributions_. Summarising our data in this way is not only useful for checking the assumptions that many models hold about our data but can also be useful to perform _coherence_ (or _confidence_) checks when evaluating the output of the more complex models we use later on.\n",
    "\n",
    "<a id='quality_check'></a>\n",
    "## Data Quality Check\n",
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many missing and unique values there are per column\n",
    "features=data.columns.values\n",
    "for feature in features:\n",
    "    print(feature,'- Missing:', \n",
    "          sum(data[feature].isnull()),\n",
    "          '- Unique:', len(data[feature].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we can improve our model by not dropping rows with missing data, but instead trying to impute what the missing values should be.  _Scikit_ has a whole set of functions for this (such as imputing the median or a fixed number), but we will create our own functions in this case because it makes it more clear what we are doing. \n",
    "\n",
    "It is important to remember that by imputing data into empty fields we are almost always making assumptions about the structure of the data. These assumptions can be more or less valid. For example, to impute genre, we may choose series ID or we may choose broadcast hour. If we impute from series ID we are making the assumption that all programs within a specific series will have the same genre. Alternatively, if we impute from broadcast hour we are assuming that all shows broadcast at a specific hour of the day are the same genre. You must judge for yourself whether these assumptions are valid or not. \n",
    " \n",
    "Since the field `genre` has only a small number of possible values but quite a large number of missing values, we will start by imputing it to reduce the amount of missing data. After that the most useful column to impute is probably `programme_duration`, again because it is quite easy to impute and will remove a fair amount of missing data. \n",
    "\n",
    "We could also think about imputing `series_id` as well if we wanted to get really clever at a later stage (maybe the same programme sometimes has a `series_id` and sometimes it doesn't). \n",
    "\n",
    "For the other missing variables imputation probably makes little sense at this stage. \n",
    "\n",
    "Again, usually we would cycle through the whole process of imputing data > building features > building/training models > evaluating models multiple times. By having a clear success metric, it makes it much easier for us to see whether any of the additional complexity delivers value.\n",
    "\n",
    "#### Genre\n",
    "\n",
    "Let's see first whether we can guess the genre based on the id of the piece of content.\n",
    "\n",
    "Here we are going to use the `pandas` library. It's one of the most useful Python library to handle databases. Here is a nice notebook with examples for some major functionalities: http://nbviewer.jupyter.org/github/rasbt/python_reference/blob/master/tutorials/things_in_pandas.ipynb#Sorting-and-Reindexing-DataFrames\n",
    "\n",
    "The pivot table will allow us to create a new database in which each row represents a `series_id`, each column is a `genre`, and we count (function _len_) the number of `streaming_id` recorded in this particular genre for this particular piece of content. We then keep for each row (piece of content), the column (genre) with the maximum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a data frame that maps each piece of content with its genre\n",
    "series_mapping=pd.pivot_table(data,values='streaming_id',\n",
    "                              index=['series_id'],\n",
    "                              columns=['genre'],\n",
    "                              aggfunc=len).idxmax(axis=1)\n",
    "series_mapping=pd.DataFrame(series_mapping).reset_index().rename(columns={0:'enriched_genre'})\n",
    "series_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's merge in the genre back into the data\n",
    "data=pd.merge(data,series_mapping,how='left',on='series_id')\n",
    "data[data['genre'].isnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many missing and unique values for this column\n",
    "features=['genre','enriched_genre']\n",
    "for feature in features:\n",
    "    print(feature,'- Missing:',sum(data[feature].isnull()),\n",
    "          '- Unique:', len(data[feature].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So while this has made things better (compare `genre` to `enriched_genre`), we are still missing the genre for a lot of different rows of data.\n",
    "\n",
    "Let's try to infer the genre based on seasonality patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What genre is the most popular at which time of day\n",
    "genre_by_time=pd.pivot_table(data,values='streaming_id',\n",
    "                             index=['time_of_day'],\n",
    "                             columns=['enriched_genre'],\n",
    "                             aggfunc=len).idxmax(axis=1)\n",
    "genre_by_time=pd.DataFrame(genre_by_time).reset_index().rename(columns={0:'enriched_genre'})\n",
    "genre_by_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the time of the day isn't granular enough. Let's consider the hour the content is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What genre is the most popular at which hour of day\n",
    "# Rmk: we first need to build this new feature\n",
    "data['hour']=data.start_date_time.apply(lambda x:x.hour)\n",
    "genre_by_hour=pd.pivot_table(data,values='streaming_id',\n",
    "                             index=['hour'],\n",
    "                             columns=['enriched_genre'],\n",
    "                             aggfunc=len).idxmax(axis=1)\n",
    "genre_by_hour=pd.DataFrame(genre_by_hour).reset_index().rename(columns={0:'enriched_genre_hour'})\n",
    "genre_by_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this granularity we can get the news pattern in the morning and the movies at night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's merge in the genre back into the data\n",
    "data=pd.merge(data,genre_by_hour,how='left',on='hour')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that imputes a genre based on the series id or the time of day if data is missing\n",
    "def impute_genre(row):\n",
    "    if isinstance(row['genre'],str):\n",
    "        return row['genre']\n",
    "    elif isinstance(row['enriched_genre'],str):\n",
    "        return row['enriched_genre']\n",
    "    else:\n",
    "        return row['enriched_genre_hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the dataframe\n",
    "data['enriched_genre']=data.apply(impute_genre, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many missing and unique values for this column\n",
    "features=['genre','enriched_genre']\n",
    "for feature in features:\n",
    "    print(feature,'- Missing:',sum(data[feature].isnull()),\n",
    "          '- Unique:', len(data[feature].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programme duration\n",
    "\n",
    "Now we can impute the length of a programme as well. The assumption we are making here is that the length of programmes of the same genre should be similar. \n",
    "\n",
    "Since our data is unlikely to be normally distributed we will use the median instead of the mean as our \"average\" programme duration.\n",
    "\n",
    "Here we use the `groupby` _pandas_ function which is really convenient to aggregate data according to a given field and given metrics. For more details: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether this has worked\n",
    "data[data['genre'].isnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the average length of a programme based on genre \n",
    "# Rmk: we rename the columns for merging afterwards\n",
    "length_by_genre=data.groupby('genre')['programme_duration_mins'].median()\n",
    "length_by_genre=pd.DataFrame(length_by_genre).reset_index().rename(columns={'programme_duration_mins':\n",
    "                                                            'enriched_duration_mins',\n",
    "                                                            'genre':'enriched_genre'})\n",
    "length_by_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's merge in the duration back into the data\n",
    "data=pd.merge(data,length_by_genre,how='left',on='enriched_genre')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that imputes a duration based on the (enriched) genre\n",
    "def impute_duration(row):\n",
    "    if row['programme_duration_mins']>0:\n",
    "        return row['programme_duration_mins']\n",
    "    else:\n",
    "        return row['enriched_duration_mins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the dataframe\n",
    "data['enriched_duration_mins'] = data.apply(impute_duration, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether this has worked\n",
    "data[data['programme_duration_mins'].isnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many missing and unique values for this column\n",
    "features=['programme_duration_mins','enriched_duration_mins']\n",
    "for feature in features:\n",
    "    print(feature,'- Missing:',sum(data[feature].isnull()),\n",
    "          '- Unique:', len(data[feature].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "Let's now have a look at the unusual values.\n",
    "\n",
    "#### Watched time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.time_viewed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some outliers here: negative values for the watched time. We will consider that these values are errors and take the absolute value to make them positive. We also could have set these values to 0. There is unfortunately no peferct way of handling errors in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that is error and make positive\n",
    "data['time_viewed']=abs(data['time_viewed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of minutes watched\n",
    "\n",
    "We built `min_watched` based on `time_viewed`. We need thus to recreate this variable as we underlined outliers for this field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['min_watched']=data['time_viewed']/(60000.0)\n",
    "data.min_watched.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percentage completion of a programme\n",
    "\n",
    "Now that we have an enriched duration (no more missing values) and a watched time field corrected from its outliers, we can calculate how much (in %) of the programme the viewer watched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of content watched by the user\n",
    "data['percentage_watched']=(data['min_watched']/data['enriched_duration_mins'])\n",
    "data.percentage_watched.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again there are wrong values (percentage >1) that we need to correct. We will set the maximum for this field to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the percentage field from when the number is more than 100%\n",
    "def clean_perc(number):\n",
    "    return min(number,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up percentaged watched from anything bigger than 100%\n",
    "data['percentage_watched']=data['percentage_watched'].apply(clean_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at our data after this imputation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many missing and unique values there are per column\n",
    "features=data.columns.values\n",
    "for feature in features:\n",
    "    print(feature,'- Missing:', \n",
    "          sum(data[feature].isnull()),\n",
    "          '- Unique:', len(data[feature].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scope'></a>\n",
    "## Scope\n",
    "\n",
    "We will work on a week time-basis for the modeling. Python weeks start on Monday and since the 1st of January was a Sunday we will ignore this day for the rest of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 1st of January - incomplete week\n",
    "data=data[data['start_date_time']>'2017-01-02']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='basic_statistics'></a>\n",
    "## Descriptive Statistics\n",
    "\n",
    "### Distributions\n",
    "Now that our data looks quite reasonable, let's plot what we have to get a better visual understanding of our feature distributions. This helps us to perform some quick _confidence checks_ and make sure the data makes sense.\n",
    "\n",
    "#### Programme duration\n",
    "\n",
    "For this _histogram_, on the x-axis we have the programme length in minutes and on the y-axis we have the number of shows for that duration that we _observe_ in our data. Note that here a given show can be counted more than once as we are considering the raw dataset of all the observations and not with a granularity of individual content IDs.\n",
    "\n",
    "The plot shows a _long-tailed_ distribution; it indicates that the vast majority of watched shows last less than a minute. These are likely to be short clips. We also see spikes around 30 and 60 minutes, which make sense as when considering the length of most shows broadcast on BBC. Interestingly, in this dataset, there are more watched shows that last an hour than last half an hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of our numerical values\n",
    "data['programme_duration_mins'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of minutes watched\n",
    "\n",
    "We should expect that the distribution of the number of minutes watched should loosely reflect the distribution of the programme duration. Indeed, from the plot below we can see a large spike at 1 minute and then two smaller spikes around the 30 and 60 minute marks. However, there are also differences between the two distributions. Compared to the 30 and 60 minutes spikes, the 1-minute spike is much larger for the number of minutes watched than than for the programme duration.\n",
    "\n",
    "One hypothesis for why this might be is that, more often than not, people do not finish watching what they started. The 1-minute column below is not only made up of viewings of 1-minute shows, but in addition, it represents all the times that people started watching something and quickly switched off or over to something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the distribution for length watches (under 200 mins to be able to see more)\n",
    "data[data['min_watched']<200]['min_watched'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percentage completion of a programme\n",
    "\n",
    "We can test our hypothesis by having a look at the distribution that shows how much the user watched during their viewing. We can see from the plot displayed below that, indeed, about 32% (~16k of a total ~50k) of the total views are represented in the very first bar (there are 50 bars so each bar represents 2% of the show). This represents evidence that it is consistent with out hypothesis from the previous section. \n",
    "\n",
    "The plot also indicates that only about 16% of the views reached the very end of the show. However, at this point we have to remember that all of the outliers (i.e. all of those values over 1 - or 100%) were rounded down to 1. This means that the final column will be artificially inflated due to this rounding, although the overall pattern is not likely to be too different.\n",
    "\n",
    "The remaining 52% stopped watching somewhere in between. We can see that the likelihood of a viewer stopping drops significantly after the first 2% of the show (first bar). Then, the likelihood of a viewer stopping at any given time continues to drop until around the 80% completion mark, at which point it it begins to sharply rise again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the completion rate\n",
    "data['percentage_watched'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two week groups\n",
    "\n",
    "Next, let's have a look at the amount of data in our dataset over time.  The _bar plot_ below of the value counts  show that all 2-week periods contain between 5.5k and 6.5k of views, apart from group 0, which contains just over 3k of views.\n",
    "\n",
    "Generally, if we assume equivalence between groups, we hope to have similar counts of data within those groups. If one group unexpectedly has a significantly different amount of data, this might be an indication of something strange happening during the sampling process. This is not necessarily the case here but it is worth considering why it does so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether we have more or less the same amount of data for each two week period\n",
    "data['twoweek'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genre and enriched genre\n",
    "\n",
    "To see what type of shows are the most popular with our viewers, we can plot the distribution of genres in our dataset. This gives us an idea of what kind of content people are consuming the most.\n",
    "\n",
    "Here, it looks like \"Factual\" and \"Drama\" shows are consumed at a much higher rate than any of the other genres. It is worth noting, however, that does not necessarily mean that \"Factual\" and \"Drama\" shows are the more popular than \"Entertainment\" shows. An alternative and equally valid interpretation of the data here is that there are more \"Factual\" and \"Drama\" shows on iPlayer than any other genre. If this were the case, there would always likely be more views in these categories, regardless of popularity. Can you think of a way to assess the popularity of a genre, independently of the volume of content? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check distribution of categorical values\n",
    "data['genre'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can learn more about our imputation process by comparing the distribution of genres before and after imputation. Below we plot the genres after imputation.\n",
    "\n",
    "Ideally, the shape of the distribution of genres, before and after imputation, will not change. If it does not change, this is an indication that imputation process has not significantly changed the original dataset and therefore, you have not introduced any bias (although the original dataset may still contain biases). If the distribution has changed after imputation (here it looks like _Factual_ has become more viewed after imputation), further consideration is needed.\n",
    "\n",
    "A change in the distribution is an indication that the missing data you have imputed was not uniformly distributed throughout the dataset (e.g. perhaps the _Factual_ producers rarely tag their content with a genre) or your imputation process is flawed and has introduced some systematic bias. As the idea that a certain type of content might be less well tagged than another is a plausible one, we needn't worry too much here for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does this differ from the distribution of the enriched genre?\n",
    "data['enriched_genre'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the difference between the genre and enriched genre.\n",
    "diff = data['enriched_genre'].value_counts() - data['genre'].value_counts() \n",
    "diff.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time of the day\n",
    "\n",
    "As a further confidence check we will look at what times of the day people are consuming the content. As one might expect the most views are recorded in the evening, then the afternoon, while the least views are recorded during the morning and night. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When do people mostly watch (distribution over time of day)?\n",
    "data['time_of_day'].value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the distribution of the programme views across the day with a distribution of the number of minutes consumed across the day (below), we can get some insight into the typical length of a programme at different times in the day.\n",
    "\n",
    "The observation that, in the number of minutes viewed distribution, _Afternoon_ and _Morning_ are more equal than in the previous plot may indicate that the average show in the morning is longer than the average show in the afternoon. As extended breakfast TV shows often dominate the early morning schedule and daytime TV shows are more discretised, this interpretation is plausible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['time_of_day'])['min_watched'].sum().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time of the week\n",
    "\n",
    "Next we shall observe how viewing trends vary on a less granular timescale; the days of the week. What we find is that viewing figures decline steadily from Monday until Friday, improve on Saturday, then reach their peak every Sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution over the weekday (weekday_0 is Monday)\n",
    "data['weekday'].value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['weekday'])['min_watched'].sum().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the equivalent plot for the number of minutes consumed (above) we see a very similar pattern to total views, providing little evidence that the average length of a show varies from day to day.\n",
    "\n",
    "One caveat to this statement is the bump of minutes consumed we see on Wednesday. This is an indication that on Wednesdays people are watching more minutes than would be expected from the number of views. This could be a consequence of longer shows being broadcast on a Wednesday or an alternative hypothesis is that people choose to watch longer shows on a Wednesday, perhaps due to fatigue. Of course, these two hypotheses are not mutually exclusive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time of the week and time of the day\n",
    "\n",
    "Finally, let's have a look to see if the daytime distributions change of the course of the week.\n",
    "\n",
    "From looking at the blue plots below it appears that there are very few differences to observe. However, by looking at the difference between the average views per day and views from a specific day (the grey plots), we can get further insight upon why variation in views exist across days.\n",
    "\n",
    "For example, if we look at this plot for weekday 0 (Monday), we can see that Monday is distinct from the average day in that there are more views in the evening and afternoon, less views in the morning and about the same at night. If we look at the weekend, we can see that Saturday has slightly more views in the morning, afternoon and night but quite a lot less evening views than the average day of the week (possibly due to people socialising on a Saturday night). On Sunday, we see many more views overall than the average day, but this is  especially the case in the morning. As these observations seem to make sense, we have a much greater confidence in our dataset than we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at whether people watch at different times across different days\n",
    "averageCounts=data['time_of_day'].value_counts()/7\n",
    "for day in sorted(set(data['weekday'])):   \n",
    "    print('\\n*********** \\n')\n",
    "    print(day+'\\n')\n",
    "    \n",
    "    data[data['weekday']==day]['time_of_day'].value_counts().plot(kind='bar')\n",
    "    plt.title(day)\n",
    "    plt.show()\n",
    "    \n",
    "    diff=data[data['weekday']==day]['time_of_day'].value_counts() - averageCounts\n",
    "    diff.plot(kind='bar',color='grey')\n",
    "    plt.title(day+\" (difference from average)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Comparing distributions and frequency counts in this way is helpful for checking whether our data is sensible but it is important to remember that these are not formal analyses. If we wanted more certain information about the difference between viewing behaviour on different days, we should use _statistical tests_ (e.g _chi-squared_) which calculate not only the difference between a measure but also the significance of that difference.\n",
    "\n",
    "### Engagement\n",
    "\n",
    "Bearing in mind that we want to forecast engagement so we should do some descriptive statistics at a user-level to obtain orders of magnitude of engagement in our data. \n",
    "\n",
    "Despite being useful business insights, it can also be used in the forecasting part to make sure our results make sense. If we forecast an average number of minutes watched in the following two weeks much higher or lower than the one observed in past data, we can raise doubts on our modeling techniques. \n",
    "\n",
    "#### Number of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of unique users in the dataset\n",
    "print('Number of users: '+\n",
    "      str(len(data.user_id.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of observations per user per two-week group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first need to aggregate the number of observations per user per twoweek\n",
    "# Describe gives us some basic statistics on this new field\n",
    "data.groupby(['user_id','twoweek']).size().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average we have 12 observations per user per two-week group. Let's plot the histogram to get a better idea of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['user_id','twoweek']).size().hist(bins=50)\n",
    "plt.title('Number of observations per user per two-week group')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that a histogram illustrates the spread in frequencies of a particular measurement. Here, that measurement is the number of content views any individual has registered over any two-week period. Each bar (or \"bucket\") represents the number of occurrences within a range of 10. As the first bucket is the tallest this is an indication that most individuals over a 2-week period will register between 0-10 views.\n",
    "\n",
    "We can thus see __various levels of engagement__ within the users on a two-weeks basis. Let's have a look at the minutes watched as well.\n",
    "\n",
    "#### Number of minutes watched per user per two-week group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first need to aggregate the number of observations per user per twoweek\n",
    "# Describe gives us some basic statistics on this new field\n",
    "data.groupby(['user_id','twoweek'])['min_watched'].sum().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the histogram to have a better idea of the distribution\n",
    "data.groupby(['user_id','twoweek'])['min_watched'].sum().hist(bins=50)\n",
    "plt.title('Number of minutes watched per user per two-week group')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is even more inequally distributed. \n",
    "\n",
    "A user can also be really engaged within a particular period of time, or on the long term. Let's try to assess the average two-weeks __retention__.  \n",
    "\n",
    "#### Number of two-week groups a user consumes content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first need to get all the possible pairs user_id x twoweek\n",
    "# And then aggregate at the user id level\n",
    "nb_twoweeks_user=data[['user_id','twoweek']].drop_duplicates()\n",
    "nb_twoweeks_user=pd.DataFrame(nb_twoweeks_user.groupby(['user_id']).size())\n",
    "nb_twoweeks_user=nb_twoweeks_user.rename(columns={0:'nb_twoweek'})\n",
    "nb_twoweeks_user.nb_twoweek.value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most of the users we only observe content consumption for one two-week group. It can be due to _churn_ (users don't come back) or because they are new iPlayer users.\n",
    "\n",
    "#### User retention on a two-weeks basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first need to get all the possible pairs user_id x twoweek\n",
    "# And then aggregate at the user id level\n",
    "nb_twoweeks_user=data[['user_id','twoweek']].drop_duplicates()\n",
    "nb_twoweeks_user=pd.DataFrame(nb_twoweeks_user.groupby(['user_id']).size())\n",
    "nb_twoweeks_user=nb_twoweeks_user.rename(columns={0:'nb_twoweek'})\n",
    "nb_twoweeks_user.nb_twoweek.value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a retention matrix\n",
    "ret_matrix=pd.DataFrame(columns=np.arange(0,9), index=np.arange(0,9))\n",
    "\n",
    "for i in np.arange(0,8):\n",
    "    # We fill [i,i] with the number of users consuming content in twoweek i\n",
    "    ret_matrix.loc[i,i]=sum(twoweeks_user['twoweek_'+str(i)])\n",
    "    twoweeks_user.cumul_tmp=twoweeks_user['twoweek_'+str(i)]\n",
    "    \n",
    "    for j in np.arange(i+1,9):\n",
    "        # Then we fill for the other columns of row i [i,j] the number of users consuming\n",
    "        # content in every twoweek from twoweek i to twoweek j\n",
    "        twoweeks_user.cumul_tmp+=twoweeks_user['twoweek_'+str(j)]\n",
    "        ret_matrix.loc[i,j]=len(twoweeks_user[twoweeks_user.cumul_tmp==j+1-i])\n",
    "        \n",
    "ret_matrix.loc[8,8]=sum(twoweeks_user['twoweek_'+str(8)])\n",
    "ret_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's put these results in ratios of the size of the cohorts\n",
    "ret_matrix_ratio=pd.DataFrame(columns=np.arange(0,9), index=np.arange(0,9))\n",
    "for i in np.arange(0,9):\n",
    "    for j in np.arange(0,9):\n",
    "        ret_matrix_ratio.loc[i,j]=ret_matrix.loc[i,j]/ret_matrix.loc[i,i]\n",
    "ret_matrix_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How to read this matrix__\n",
    "\n",
    "From all the users who consumed content in `twoweek` 0, more than 65% came back in `twoweek` 1 and around 50% are still consuming content on `twoweek` 2.\n",
    "\n",
    "Among the users who consumed content on `twoweek` 4, 73% came back the `twoweek` after. Note that for this cohort, users might have consumed content before `twoweek` 4.\n",
    "\n",
    "Only 24% of the users are consuming content during all the scope, with this time basis.\n",
    "\n",
    "If we want to get an average retention from one `twoweek` to another and not get into that much details, we can average the retention ratio observed for one two-week group to the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute an average retention ratio\n",
    "ret_tw=[]\n",
    "for i in np.arange(0,8):\n",
    "    ret_tw+=[ret_matrix_ratio.loc[i,i+1]]\n",
    "print('Average retention from one twoweek to another: '+str(np.mean(ret_tw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retention is quite high in this user database from one two-week group to another.\n",
    "\n",
    "Now that we have a better understanding of our data, assess its quality and build some extra features, we can save this new dataset in order to read it back in for the next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('iplayer_data_c1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this tutorial we have walked through some of the early steps involved in many data science problems: \n",
    "1. Formulating our problem;\n",
    "2. Selecting the data we need to solve it;\n",
    "3. Preparing and cleaning our data to make it easier to analyse\n",
    "4. Exploring and making sense of our data,\n",
    "\n",
    "In the next tutorial we will prepare the dataset that will feed into the modelling part."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
