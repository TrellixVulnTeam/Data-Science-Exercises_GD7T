{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to document embeddings\n",
    "\n",
    "Natural Language Processing is a very useful field in the realm of Data Science. It can help you understand large corpora, and leverage decisions from it.\n",
    "\n",
    "In this workshop we'll try to introduce you to the basics of NLP, in particular document embeddings. This notebook is meant to be an initial approach to the subject, and if everything works well, we expect you to be curious enough to continue with the challenges we propose at the end.\n",
    "\n",
    "The question we'll be trying to answer in this workshop is _Can we use document embeddings to understand large corpora?_\n",
    "\n",
    "We assume that for this workshop you have already set-up your environment to be able to work with this notebook, and know how to code in Python. If you have not set up properly the environment, or do not know what is a Jupyter Notebook, we suggest you take a look [here](https://github.com/bbc/datalab-ml-training).\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load some libraries that we will use throughtout this notebook. There is no need to know all of them at this stage. We'll go into more details when we'll need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from utils.load import TextFiles\n",
    "from matplotlib.colors import ListedColormap\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from numpy.random import randint\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBC Dataset\n",
    "\n",
    "In this workshop we will be using the [BBC Dataset](http://mlg.ucd.ie/datasets/bbc.html): a public set of 2,225 labeled articles originated from BBC News, from 2004 to 2005, that can be used for machine learning research. The articles are divided in five categories:\n",
    "- Business\n",
    "- Entertainment\n",
    "- Politics\n",
    "- Sport\n",
    "- Tech\n",
    "\n",
    "We'll start our analysis from the [raw files from the BBC Dataset](http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip). You can find the files in the `bbc_news` folder.\n",
    "\n",
    "### What does an article look like?\n",
    "Let's start by looking at one article, to try and understand how the files look like. Here the first business article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bbc_news/business/001.txt', 'r') as fp:\n",
    "    raw_data = fp.read()\n",
    "\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the file is well structured. We have a headline (or title) in the first line, followed by the article text below, where each paragraph is separated by two new line delimiters (`\\n`).\n",
    "\n",
    "In your DS career, you'll probably deal with text data way less cleaner than this dataset. In this case, you'll have to pre-process the original file, in order to get to a similar state.\n",
    "\n",
    "### Let's now load all the files\n",
    "For our example, the articles fit inside our memory, and we can create a function to load them all: `read_files_from_folder` navigates inside the folder tree, and returns a list of all raw texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_from_folder(root_folder):\n",
    "    contents = []\n",
    "    for root, folder, file_names in os.walk(root_folder):\n",
    "        for file_name in file_names:\n",
    "            if file_name.endswith('.txt'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                with open(file_path, 'r') as fp:\n",
    "                    contents.append(fp.read())\n",
    "    return contents\n",
    "\n",
    "raw_contents = read_files_from_folder('bbc_news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ERROR!**\n",
    "\n",
    "What happened there? There are some files that were not encoded using 'UTF-8', so whenever we try to load them, we face a 'UnicodeDecodeError'. The best thing to do is to fix those files at creation, but sometimes that's not an option.\n",
    "\n",
    "There's a trick, that works most of the time: using chardet to detect the file encoding. _BUT use with caution._\n",
    "\n",
    "To make this workshop a bit faster, we've built a small class named `TextFiles` to deal with this issue and load the text files in memory. We'll use a function that also returns along with the raw text, the category of the article: output corpus being a list of dictionaries with the article (key `raw_content`) and the category (key `category_1`).\n",
    "\n",
    "To tailor to our needs, we've also added the possibility to blacklist some files when creating our corpus (parameter `ignored_files` that takes a list of file names).\n",
    "\n",
    "Have a look it the /lib folder if you want to see how it is built. And if you like this idea, we invite you to read how [Spark](https://spark.apache.org/docs/latest/) handles this, with the function [textFile](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=textfile#pyspark.SparkContext.textFile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = TextFiles().read_folder_with_categories('bbc_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[randint(0, len(raw_data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset exploration\n",
    "Now that we have loaded all our articles, let's get some basic statistics. It's important to get a good overview of the data we are dealing with as it will drive us in the right direction for the processing, for the algorithms to consider etc. For example, we might need different models when dealing with tweets versus large documents.\n",
    "\n",
    "Let's try to answer some of the following questions:\n",
    "- Is our Dataset balanced? How many documents do we have per category?\n",
    "- Are some files empty?\n",
    "- Are there any duplicates?\n",
    "- How many words are usually used? And paragraphs? And lines?\n",
    "- What about the headline? How long are they?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of documents per category\n",
    "How many articles do we have per category? If the distribution is unbalanced, and we are working on a classification problem, it might be needed to undersample some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "folder = 'bbc_news'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how many documents we have per category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categories seem to be balanced. Yay!\n",
    "\n",
    "#### Empty files and small articles\n",
    "Let's check if we have any empty files in our dataset, or files that are very small. If we have any, we should remove them and that something we should handle when loading the files, i.e. in the `TextFiles` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have any small or empty articles in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's good to know, at least there are no empty files in this dataset.\n",
    "\n",
    "#### Duplicates\n",
    "We need to get rid of the duplicates as they are not giving us extra information. The definition of similarity or duplication can be discussed, here we consider only exact duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here to detect how many duplicates we have in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check now how many duplicates we have per category, as it can impact the balance of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to create a dataset that contains the duplicates per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a new stacked chart with the duplicated items per category along with the uniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the majority of the duplicates are in tech. If we train a classification model based on this dataset, we might need to balance our corpus.\n",
    "\n",
    "Let's keep that in mind for the rest of our work.\n",
    "\n",
    "### Data processing\n",
    "When dealing with text data, we usually apply few transformations. First, we don't work with the raw text but with tokens. Also we usually remove stopwords, and we consider only the root of the works: which can be done by stemming, or lemmatization.\n",
    "\n",
    "Finally, some work could be done to replace things like numbers, names, locations, organisations with known tokens. If we have a robust embedding model, it shouldn't matter if the person we're talking about is Theresa May, or Madonna: the context should tell us what the document is about.\n",
    "\n",
    "We'll keep this for later iterations.\n",
    "\n",
    "#### Stemming or Lemmatization?\n",
    "Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word. This is easier for interpretation.\n",
    "\n",
    "Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, a WordNet corpus is used, which makes it slower than the stemming process. To obtain the correct lemma, you also have to define the concept in which you want to lemmatize, or so called \"parts-of-speech\".\n",
    "\n",
    "For a good introduction for both concepts, see [here](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python).\n",
    "\n",
    "Let's now try out these concepts with a concrete example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_raw_text = \"TimeWarner is to restate its accounts as part of efforts to resolve \\\n",
    "an inquiry into AOL by US market regulators. It has already offered to pay $300m to \\\n",
    "settle charges, in a deal that is under review by the SEC. The company said it was \\\n",
    "unable to estimate the amount it needed to set aside for legal reserves, which it \\\n",
    "previously set at $500m. It intends to adjust the way it accounts for a deal with \\\n",
    "German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had \\\n",
    "reported as advertising revenue. It will now book the sale of its stake in AOL Europe \\\n",
    "as a loss on the value of that stake.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(ex_raw_text.lower())\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_stop_words = filter(lambda t: t not in stop_words, tokens)\n",
    "list(tokens_no_stop_words)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = filter(lambda t: t not in stop_words, tokens[:20])\n",
    "print(\"{0:20}{1:20}{2:20}{3:20}{4:20}\".format(\"Word\", \"Porter Stemmer\",\n",
    "                                              \"Lancaster Stemmer\", \"Lemmatizer\",\n",
    "                                              \"Lemm pos=v\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}{3:20}{4:20}\".format(\n",
    "        word,\n",
    "        porter_stemmer.stem(word),\n",
    "        lancaster_stemmer.stem(word),\n",
    "        wordnet_lemmatizer.lemmatize(word),\n",
    "        wordnet_lemmatizer.lemmatize(word, pos=\"v\") \n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply these transformation to our corpus. We'll start by using the Porter Stemmer to get the root of each word. In order to apply easily different transformations, we recommend to use Pipelines (an example of how to build a transformation is under `utils/preprocessing.py`, and you can plug that into a [Pipeline object](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(raw_content):\n",
    "    # Create a function to preprocess the text and return the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in raw_data:\n",
    "    article['tokens'] = process_text(article['raw_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why embeddings?\n",
    "Whenever we're building a model to capture how similar two documents are, we need to consider how we are representing the document.\n",
    "\n",
    "For example, depending on the technique we use for processing the text, the three phrases below would have the same coded values (or tokens).\n",
    "\n",
    "(1) This is completely meaning**ful** to me.\n",
    "\n",
    "(2) This is completely meaning**less** to me.\n",
    "\n",
    "(3) To be complete: this is meaningful to me. \n",
    "\n",
    "After removing stopwords, tokenizing and stemming, we might end in the same vector representation, because some techniques depend on the tokens alone and do not take into consideration the context.\n",
    "\n",
    "This is something that we definitely do no want, what lead us to use techniques like [Non-Negative Matrix Factorization](https://arxiv.org/abs/1401.5226), Latent Dirichlet Allocation (LDA), and Document Embeddings (like [Doc2Vec](https://arxiv.org/pdf/1405.4053v2.pdf), [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) and [ELMo](https://allennlp.org/elmo)). In some cases, simpler models can be sufficient, but usually only when we have _lots of data_.\n",
    "\n",
    "With embeddings, in particular, we are able to sum and subtract the word vectors. A well known example is:\n",
    "_QUEEN_ - _WOMAN_ + _MAN_ = _KING_.\n",
    "\n",
    "This is closer to what we would expect from a language model!\n",
    "\n",
    "For the embeddings we consider the [Doc2Vec algorithm](https://radimrehurek.com/gensim/models/doc2vec.html) from the Gensim library, similar to [here](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb). This is based on two papers that used either [hierarchical softmax](https://arxiv.org/pdf/1301.3781.pdf) or [negative sampling](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf).\n",
    "\n",
    "In order to use it, we first need to convert our tokens into [TaggedDocument](https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.TaggedDocument) objects. Tagged documents are documents with an associated label. You could simply use one number for each document and run the model, but when more than one document contains the same label, the model will try to infer the information from documents with the shared tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_documents = [\n",
    "    TaggedDocument(list(elnt['tokens']), [f'article_{idx}']) for idx, elnt in enumerate(raw_data)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_documents[randint(0, len(tagged_documents))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to play with the hyperparameters here, to get a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to set the proper hyperparameter models here\n",
    "doc2vec_model = gensim.models.Doc2Vec(tagged_documents, dm=1, vector_size=5, window=5,\n",
    "                                      alpha=0.025, min_alpha=0.025, min_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This won't be good the first time you run. You need to change the hyperparameters\n",
    "print(doc2vec_model.wv.most_similar('court'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, article in enumerate(raw_data):\n",
    "    article['embeddings'] = doc2vec_model.docvecs[f'article_{idx}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the embeddings were added to the document\n",
    "print(raw_data[randint(0, len(raw_data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "So we've reached the end of our workshop, but this is just the beginning. From this point onwards, we leave to you some open questions, that may guide on your journey.\n",
    "\n",
    "- Did you notice that the model is saving some specific names? How would the model perform if you replace named entities with placeholders?\n",
    "- How would you check the model above for different types of bias? How would you check, for example, if the model has a gender or race bias?\n",
    "- What would be the benefits of plugging the process from above into a Pipeline object?\n",
    "- What would be the differences in implementation between the Pipeline you can create here for what you have in Apache Beam or Spark?\n",
    "- How would you refactor this without using Gensim, and using Keras or Tensorflow instead?\n",
    "- What would need to change in your code if you'd like to train this model with data that doesn't fit in memory of a single machine?\n",
    "- How can you detect the minumum number of articles needed to train your model for your use case?\n",
    "- Would you use cross validation when training this model? Why and how?\n",
    "- Is there a way to use Bayesian Optimization, or a simple grid search to find the best hyperparameters?\n",
    "- How would you compare Doc2Vec with LDA, NNMF, BERT or ELMo?\n",
    "- If your problem was to just classify the text into multiple categories, what would you do differently?\n",
    "- How can you make this model available as a service?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
