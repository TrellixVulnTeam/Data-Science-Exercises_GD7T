{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Three: Build a Classifier\n",
    "\n",
    "Expected time to complete: 1 hour\n",
    "\n",
    "## Goal of this Course\n",
    "During this course we will build a classifier to forecast the minutes watched within the last two-week group.\n",
    "\n",
    "This course is split into the following parts:\n",
    "- <a href='#context'>Context</a> \n",
    "- <a href='#estimation'>Estimate the Parameters</a> \n",
    "- <a href='#training_test'>Training and Test Sets</a>\n",
    "- <a href='#model_selection'>Model Selection</a>\n",
    "- <a href='#model_evaluation'>Model Evaluation</a>\n",
    "- <a href='#baseline'>Baseline Score</a>\n",
    "    - <a href='#load_data'>Load the Data</a>\n",
    "    - <a href='#def_baseline'>Define a Baseline</a>\n",
    "\n",
    "\n",
    "- <a href='#decision_tree'>Decision Tree</a>\n",
    "    - <a href='#training'>Model Training and Hyperparameters Tuning</a>\n",
    "    - <a href='#best_decision_tree'>Understanding our Best Decision Tree Model</a>\n",
    "\n",
    "\n",
    "- <a href='#rf'>Random Forest</a>\n",
    "    - <a href='#training2'>Model Training and Hyperparameters Tuning</a>\n",
    "    - <a href='#best_rf'>Understanding our Best Random Forest Model</a>\n",
    "    \n",
    "    \n",
    "- <a href='#logistic_reg'>Logistic Regression</a>\n",
    "    - <a href='#training3'>Model Training and Hyperparameters Tuning</a>\n",
    "    - <a href='#best_logistic_reg'>Understanding our Best Logistic Regression Model</a>    \n",
    "\n",
    "\n",
    "- <a href='#svm'>Support Vector Machine</a>\n",
    "    - <a href='#training4'>Model Training and Hyperparameters Tuning</a>\n",
    "    - <a href='#best_svm'>Understanding our Best SVM Model</a>\n",
    "\n",
    "\n",
    "\n",
    "- <a href='#model_perf_comp'>Model Performance Comparison</a>\n",
    "\n",
    "\n",
    "<a id='context'></a>\n",
    "# Context\n",
    "\n",
    "In the previous course we had a look at the process of preparing our dataset so that it can be easily ingested into one of many statistical learning models. We looked at how to identify whether our data is suitable for a supervised or unsupervised learning framework and also the differences between preparing our data for classification or regression models.  \n",
    "\n",
    "In this course we will introduce key concepts around the use of machine-learning models while demonstrating the way a classification model can be used with a dataset. We will look at the process of choosing the most appropriate model, the way to train it with your data and finally how to evaluate it's performance.\n",
    "\n",
    "<a id='estimation'></a>\n",
    "# Estimate the Parameters\n",
    "\n",
    "As we are in the supervised framework, the objective of each model we use for this project will be to formalise a relationship between the target output _y_ - what we want to forecast - and the the observable features _X_, using more or less complex formulas. These formulas will contain parameters that we need to estimate.\n",
    "\n",
    "For all algorithms we will minimise the  _loss function_ - this quantifies the distance between our model predictions (with a specific set of parameters) and the correct output _y_. The estimation process can be tricky as we are considering complex relations between our features and output.  Fortunately, we do not need to know much more than this as Python is doing all the maths for us! \n",
    "\n",
    "For those interested in learning more about optmisation and, in particular, the popular _gradient descent_ algorithm, there is a nice post here that uses excel to demonstrate: https://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html\n",
    "\n",
    "<a id='training_test'></a>\n",
    "# Training and Test Sets\n",
    " \n",
    "When training machine learning models - i.e. estimating their parameters - we want to avoid training the model on all of the possible data that we have available. This is to avoid creating a model that is tuned too specifically to our training data and will later not generalise - this is often called _overfitting_. \n",
    "\n",
    "So, instead we will spilt our data into _training sets_ and _test sets_. We then estimate the parameters of the model using the training sets and then evaluate performance using the test sets.  A popular approach is  _k-fold cross-validation_. Cross-validation is a clever way of carving up our dataset so that we can use all of it for training while still allowing us to evaluate or model using unseen data.   It works by initially splitting the dataset into  _k_ equally-sized sets (or \"folds\"). Then we loop through each of the _k_ folds of data (*k_i*) and do the following: \n",
    "\n",
    "- Remove fold *k_i* and label that the test set.\n",
    "- Aggregate the remaining *k-1* folds and label that the training set.\n",
    "- Train the model using the data from the training set. \n",
    "- Evaluate the performance (compute the error) of the trained model using data from the test set.\n",
    "\n",
    "Once we have trained all of our _k_ models we have computed _k_ errors or performance metrics (see the model evaluation part for more details on the metrics used). The average of these errors has been found to be a more robust metric than any of the individual error scores.\n",
    "\n",
    "For many complex problems and datasets the _bleeding_ of information from the test set into the training set by mistake can be a real problem. In such a case, our model would perform much worse in production than what we would otherwise expect. Very few data scientists have avoided the scenario where initially, they think they have trained a near perfect model and think they are a genius, only to realise later that there was a bug in the code and a leak of information between the training and test set exists.\n",
    "\n",
    "Tip: _if your model performance looks too good to be true - it probably is_.\n",
    "\n",
    "<a id='model_selection'></a>\n",
    "# Model Selection\n",
    " \n",
    "The first decision you need to make in the model selection process within the supervised framework is whether you plan to use a _classifier_ or a _regressor_ model. As discussed in the previous course, classifiers make discrete predictions about a datapoint into a finite number of classes while regressors make linear predictions.  \n",
    " \n",
    "Different models work in different ways and are more or less suitable for different problems. Fortunately, however,  understanding these specific differences is not essential to solve your data problems. The python module `scikit learn` contains all of the models that you are likely to need and the format of the data it requires is standardised across models. This makes it very easy to try your data using a myriad of different models and choose the one that performs best on your data.\n",
    " \n",
    "In our current project we use both classifiers and regressors to predict engagement. In the classifier we simply try to predict whether someone has watched any content in the two-week period, while in the regressor we attempt to predict the number of minutes watched by the viewer within the two-week period.\n",
    "\n",
    "We won't tackle both prediction tasks here. This course focuses on classification and the next one will focus on regression.\n",
    "\n",
    "<a id='model_evaluation'></a>\n",
    "# Model Evaluation\n",
    "\n",
    "To evaluate our models there are different metrics we can use. For classifiers, two popular metrics are _accuracy_ (percentage correct) and _the ROC curve_. The accuracy is the simplest metric but it gives us little insight into the behaviour of the model. ROC curves (and _the area under the curve_ statistic) give us a greater understanding of the separability of the data. For more details see https://en.wikipedia.org/wiki/Receiver_operating_characteristic.\n",
    "\n",
    "We will evaluate our model at different stages in the model selection process. First, when training our model we will compute a test, or _out-of-sample error_, (actually _k_ errors after cross-validation). It's known as out-of-sample here because the predictions are made on data the model has never seen before. \n",
    "\n",
    "Usually models have also _hyperparameters_; these are parameters of the learning algorithms we use to train our model (parameters of the parameter-estimators). To find the best combination of hyperparameters, we usually try many different combinations (or perform a _grid search_ ) and select the combination that yields the lowest error. \n",
    "\n",
    "Once we have selected the best set of hyperparameters for a given model, or the best kind of model for the data we have got, we can retrain a new model on the entire dataset and compute its _in-sample_ error; i.e. evaluate its predictions with the observed targets, keeping in mind that we have used this data to build the model this time. \n",
    "\n",
    "The most unbiased performance metric of a model will be it's out-of-sample error, but on a set of data that has not been used in any of the training phase, including the process of during hyper-parameter search. It is for this reason that it is considered best practice to hold out a further _validation set_ that can be used in lieu of the test set when searching for the optimal hyperparameter combination. This means that the _test set_ remains unaffected by the model fitting process (training and hyperparameter estimation) and can be used for a fully unbiased estimation of performance.\n",
    "\n",
    "Finally, when computing the performance of a model based on a given metric it's important to bare in mind what the performance of the most simple model would be. Usually we consider the completely random one as a baseline. Any model you build must be evaluated in terms of improvement over this performance. \n",
    "\n",
    "<a id='baseline'></a>\n",
    "# Baseline Score\n",
    "\n",
    "<a id='load_data'></a>\n",
    "## Load the Data\n",
    "Now let's dive back into the code. The first thing to do is to get our data back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Comment below if you want to display the warnings (quite verbose here in the modeling part)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We put both target arrays (regression and classification) in the same txt file\n",
    "# As both target arrays have the same size we just need to split it it two\n",
    "# and get the correct part for the prediction task\n",
    "target = np.split(np.loadtxt('target.txt'), 2)[1].flatten()\n",
    "features = pd.read_csv('features.csv')\n",
    "\n",
    "# User id as index\n",
    "features = features.set_index('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='def_baseline'></a>\n",
    "## Define a Baseline\n",
    "\n",
    "Usually dealing with a binary classification problem (1/0) we talk about __scoring__. The probability of belonging to the class 1 (usually our class of interest) is the score. Alternatively, we can always predict the most common class. This gives the highest possible accuracy without doing any actual classification, but might not make sense for your problem. It's up to you to decide which is most appropriate; but if in doubt, do both.\n",
    "\n",
    "As previously mentioned, we should always be aware of a baseline to compare the performance of our models with. We usually use as a baseline score the one that would be achieved if we were using a random model.  If, in our random model, we predict all 100/100 users to be in class 1, the accuracy of this model can be calculated as the proportion of class 1 in the entire population of 100. If we observe 30% of actual class 1 users in the population, then we would expect 30 correct predictions labeled class 1 if our model made random predictions.\n",
    "\n",
    "This approach is, of course, equally valid if we choose constant class 0 values instead of class 1 values for our random model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check our baseline score\n",
    "sum(target)/len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also check the score of the most common class, which given is target class 0\n",
    "sum(target==0)/len(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for any classification model to add value we would like it to perform with an accuracy of more than 60% (otherwise guessing based on the proportions would be a better model).\n",
    "\n",
    "We will try 4 different kinds of classification model.\n",
    "\n",
    "<a id='decision_tree'></a>\n",
    "# Decision Tree\n",
    "\n",
    "Decision-trees are one of the most simple to understand classifiers. They work on the assumption that a classification can be made based upon the answers to a series of yes/no questions about the data. When visualised in graph form, this series of questions assume the appearance of a tree, hence the name. The analogy continues with \"branches\" as answers to questions and \"leaves\" as the class labels at the end of the branches.\n",
    "\n",
    "For more details: https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "\n",
    "For all the models we use the Python `scikit learn` library, as it is really easy to use and well-documented. For decision trees see: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a simple tree based classification model\n",
    "from sklearn import tree\n",
    "\n",
    "# Accuracy as our error evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# We will use cross validation, so import helper functions for this\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training'></a>\n",
    "## Model Training and Hyperparameters Tuning\n",
    "As explained previously, we will do cross-validation during the training process. There is no need to manually split our dataset as `scikit learn` provides a function to do so. \n",
    "\n",
    "There are two hyperparameters in decision trees that we need to tune:\n",
    "- The maximum depth of the tree `max_depth` (default _None_). Think of this as the maximum number of questions the model can ask of the data before making a classification\n",
    "- The minimum number of samples required to be at a leaf node `min_samples_leaf` (default 1). This value effects the decision of whether a branch node becomes a leaf node or is split into a further two branches.\n",
    "\n",
    "Here we will keep track of the hyperparameters that maximize the accuracy (or error score). These settings will define our _best decision tree_ model. \n",
    "\n",
    "The score returned by our code is the average of the out-of-sample scores computed during the cross-validation process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model and develop a simple grid search against some key parameters\n",
    "DT_param_max_depth=[2,3,4,6,8,10]\n",
    "DT_param_min_leaf=[75,90,100,110,125,150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep track of our best parameters\n",
    "DT_best_score=0\n",
    "DT_best_param=[0,0]\n",
    "\n",
    "# We will use the itertools library to try all the possible combinations of paramaters\n",
    "# We could also have used the gridsearchCV capability in scikit learn\n",
    "for c in itertools.product(DT_param_max_depth,DT_param_min_leaf):\n",
    "    treeclass=tree.DecisionTreeClassifier(max_depth=c[0],min_samples_leaf=c[1])\n",
    "    scores=cross_val_score(treeclass,\n",
    "                           features,\n",
    "                           target,\n",
    "                           scoring='accuracy')\n",
    "    if np.mean(scores)>DT_best_score:\n",
    "        DT_best_score=np.mean(scores)\n",
    "        DT_best_param=c\n",
    "\n",
    "# Print the overall best results\n",
    "print('Best Settings: Max Depth:',DT_best_param[0],'- Min Sample Leaf:',DT_best_param[1])\n",
    "print('Score:',DT_best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this means that we would expect our best decision tree model to correctly classify just under 80% of our users as either engaged or not engaged within our two-week test period, based upon our features from the previous 14 weeks.\n",
    "\n",
    "<a id='best_decision_tree'></a>\n",
    "## Understanding our Best Decision Tree Model\n",
    "\n",
    "Now that we have identified our hyperparameters, it is often helpful to inspect this \"best\" model to try understand how it is working and discover which features are contributing the most information. To do this, we can train a new model using the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep our best model (hyperparameters tuned)\n",
    "treeclass=tree.DecisionTreeClassifier(max_depth=DT_best_param[0],\n",
    "                                      min_samples_leaf=DT_best_param[1])\n",
    "DT=treeclass.fit(features,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the in-sample accuracy. Remember that the in-sample accuracy is not a good indication of how well the model will generalise as we are evaluating it using the same data that we have trained it on. \n",
    "\n",
    "The in-sample accuracy (or inversely the \"training error\") is a useful metric to compare with the out-of-sample accuracy (or inversely the \"test\" error). Similar scores for the in-sample and out-of-sample accuracy is a good sign that the model is not overfitting. However, if we see significantly higher performance in the in-sample scores, this is a sign that our model is overfitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model has been trained on the entire dataset - i.e. the output we are comparing our\n",
    "# predictions with here have been used to estimate the parameters\n",
    "DT.score(features,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the in-sample score is very similar to the out-of-sample score, which is a positive sign that the model is not overfitting and is more likely to generalise to unseen data.\n",
    "\n",
    "To gain a greater insight into how the model is working we can rank our features by their importance to the model. `Scikit learn`  provide a way of doing this by calculating the _Gini importance_ of each feature.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features importance\n",
    "feature_impDT=pd.DataFrame(\n",
    "    {'feature': list(features.columns),\n",
    "     'importance': list(DT.feature_importances_)\n",
    "    })\n",
    "feature_impDT.sort_values(by='importance', ascending=False).reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the the most important feature for predicting iPlayer engagement in our target two-week group is the number of minutes viewed in the two previous weeks. The number of sessions within the past 14 weeks is also important, but to a much lesser extent. \n",
    "\n",
    "These first  _business_ insights sound sensible and represent an indication that the model is doing what we expect it to. \n",
    "\n",
    "<a id='rf'></a>\n",
    "# Random Forest\n",
    "A random forest is a collection of decision trees. Single decision trees as documented above are built using the entirety of the training set while random forests generate multiple random subsets from the input data and train different trees with different subsets. Different trees are therefore trained on different samples and different features. Each tree votes for a particular class and the class receiving the most votes wins. \n",
    "\n",
    "The concept of _resampling data_ that is at the heart of random forests is common in machine-learning techniques. You may have noticed how cross-validation also uses a resampling technique to make more of the dataset and generate a more robust error score. Random forests are doing something similar. Random forests are also less prone to overfitting than decision trees as the trees they grow tend to be smaller and less complex. \n",
    "\n",
    "For more details: https://en.wikipedia.org/wiki/Random_forest <br>\n",
    "\n",
    "And the `scikit learn` documentation: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a random forrest\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training2'></a>\n",
    "## Model Training and Hyperparameters Tuning\n",
    "We need to tune the same hyperparameters as for a decision tree model:\n",
    "- the maximum depth of the tree `max_depth` (default _None_);\n",
    "- the minimum number of samples required to be at a leaf node `min_samples_leaf` (default 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model and develop a simple grid search against some key parameters\n",
    "RF_param_max_depth=[2,3,4,6,8,10]\n",
    "RF_param_min_leaf=[75,90,100,110,125,150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep track of our best parameters\n",
    "RF_best_score=0\n",
    "RF_best_param=[0,0]\n",
    "\n",
    "# We will use the itertools library to try all the possible combinations of paramaters\n",
    "# We could also have used the gridsearchCV capability in scikit learn\n",
    "for c in itertools.product(RF_param_max_depth,RF_param_min_leaf):\n",
    "    forrestclass=RandomForestClassifier(n_estimators=200,\n",
    "                                        max_depth=c[0],min_samples_leaf=c[1])\n",
    "    scores=cross_val_score(forrestclass,\n",
    "                           features,\n",
    "                           target,\n",
    "                           scoring='accuracy')\n",
    "    if np.mean(scores)>RF_best_score:\n",
    "        RF_best_score=np.mean(scores)\n",
    "        RF_best_param=c\n",
    "\n",
    "# Print the overall best results\n",
    "print('Best Settings: Max Depth:',RF_best_param[0],'- Min Sample Leaf:',RF_best_param[1])\n",
    "print('Score:',RF_best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see here that the accuracy is very similar to the accuracy achieved by our privious decision tree. This means that, for this particular problem, random forests provide little benefit over a single decision tree.\n",
    "\n",
    "<a id='best_rf'></a>\n",
    "## Understanding our Best Random Forest Model\n",
    "\n",
    "Now that we have identified our best hyperparameters, let's train a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep our best model (hyperparameters tuned)\n",
    "forrestclass=RandomForestClassifier(n_estimators=200,\n",
    "                                    max_depth=RF_best_param[0],\n",
    "                                    min_samples_leaf=RF_best_param[1])\n",
    "RF=forrestclass.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-sample accuracy\n",
    "RF.score(features,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar in- and out-of sample accuracy, just like for the decision tree, is a positive indication that the model is not overfitting.\n",
    "\n",
    "Now let's look at the importance of the features in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features importance\n",
    "feature_impRF=pd.DataFrame(\n",
    "    {'feature': list(features.columns),\n",
    "     'importance': list(RF.feature_importances_)\n",
    "    })\n",
    "feature_impRF.sort_values(by='importance', ascending=False).reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the random forest distributed importance over many more features than the single decision tree did. We still see that the most recent two-week period of views as the most informative but not by so much as we saw in the original decision tree. \n",
    "\n",
    "<a id='logistic_reg'></a>\n",
    "# Logistic Regression\n",
    "\n",
    "Next let's try a classifier that is not based upon decision trees. Logistic regression is a specialised example of a _generalised linear model_.\n",
    "\n",
    "Generalised linear models estimate the parameters of the following equation: _y_ = _b1x_ + _b2_ +  _e_ where _y_ is the output variable, _x_ is a feature input, _b1_ is a parameter that weights the given input, _b2_ is an additional parameter and is _e_ is the normally-distributed error. If the model has _n_ features, the _b1x_ expression can be replaced by the sum of *x_i...n* *b_i...n*. This is otherwise known as multiple regression.\n",
    "\n",
    "Logistic regression is a special version of multiple regression as its output is categorical and not linear. The output of a logistic regression model is turned into a probability (a number between 0 and 1) by passing it through a _logistic function_. This probability then allows us to make a prediction of class assignment.  \n",
    "\n",
    "Scikit documentation: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a logistic regression\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training3'></a>\n",
    "## Model Training and Hyperparameters Tuning\n",
    "\n",
    "In the same way that we did with the decision-tree based classifiers, we shall first estimate the hyperparameters. The \"C\" hyperparameter that we shall estimate here measures the amount of _regularization_ in the model.\n",
    "\n",
    "When very large numbers are estimated for model parameters, overfitting tends to increase. Regularization is a technique that penalises the size of these estimated parameters during training. In this case, the smaller C is, the stronger the regularizarion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the model and develop a simple grid search against some key parameters\n",
    "LR_param_C=[0.001,0.01,0.1,1.0,10,100,1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep track of our best parameters\n",
    "LR_best_score=0\n",
    "LR_best_param=0\n",
    "\n",
    "# we will setup a manual grid search, but you can also use the gridsearchCV capability in sklearn\n",
    "for i in LR_param_C:\n",
    "    logclass=linear_model.LogisticRegression(C=i)\n",
    "    scores=cross_val_score(logclass,\n",
    "                           features,\n",
    "                           target,\n",
    "                           scoring='accuracy')\n",
    "    if np.mean(scores)>LR_best_score:\n",
    "        LR_best_score=np.mean(scores)\n",
    "        LR_best_param=i\n",
    "\n",
    "# print the overall best results\n",
    "print('Best Settings: C:',LR_best_param)\n",
    "print('Score:',LR_best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have found our optimal combination of hyperparameters, let's evaluate the model using all of the data.\n",
    "\n",
    "<a id='best_logistic_reg'></a>\n",
    "## Understanding our Best Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep our best model (hyperparameters tuned)\n",
    "logclass=linear_model.LogisticRegression(C=LR_best_param)\n",
    "LR=logclass.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-sample accuracy\n",
    "LR.score(features,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like all our previous models the difference between the in-sample accuracy and out-of-sample accuracy is marginal, meaning there is no sign of overfitting and the model will likely generalise well to new data.\n",
    "\n",
    "To get a better understanding of how our logistic regression model works, we can have a look at the parameter estimates that weight our individual features. We need to be careful though as linear models make certain assumptions about the data they model, which theoretically should be checked _before_ using them. One of these assumptions is that the input variables or features should not be correlated. While a violation of this assumption does not invalidate the model it does make the the parameter estimates uninterpretable.\n",
    "\n",
    "For more information on the assumption of logistic regression that are beyond the scope of this tutorial: http://www.statisticssolutions.com/assumptions-of-logistic-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's have a look at the highest magnitude positive coefficients.\n",
    "coefLR=pd.DataFrame(\n",
    "    {'feature': list(features.columns),\n",
    "     'coef': list(LR.coef_.flatten())\n",
    "    })\n",
    "coefLR.sort_values(by='coef',ascending=False).reset_index(drop=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, lets have a look at the highest magnitude negative coefficients.\n",
    "coefLR.sort_values(by='coef',ascending=False).reset_index(drop=True).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot them all\n",
    "coefLR=coefLR.set_index(['feature'])\n",
    "coefLR.sort_values(by='coef', ascending=False).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the estimated coefficients we should also in theory have a look at the _p-values_; statistic values that indicate whether our estimated coefficient is significantly different from zero. However we won't go into more detail here.\n",
    "\n",
    "The logistic regression framework seems also less suited for our data than the decision tree according to the training and testing performance. \n",
    "\n",
    "<a id='svm'></a>\n",
    "# Support Vector Machine\n",
    "\n",
    "A support vector machine (SVM) is yet another type of classifier that can be used to predict categorical output variables. Put simply, SVMs search for the _hyperplane_ (think of a high-dimensional straight line) that maximises the distances between the two classes. \n",
    "\n",
    "For more about SVMs: \n",
    "Scikit documentation: http://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to get some non linear patterns\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training4'></a>\n",
    "## Model Training and Hyperparameters Tuning\n",
    "\n",
    "Like before, we need to tune the hyperparameters of our model. For this SVM we will tune just a single hyperparameter: \"C\". \n",
    "\n",
    "This value tells our SVM how much we care about avoiding misclassification of each datapoint in the training data. For smaller C values, the model will choose a hyperplane with a larger margin between the two classes; but as a result will misclassify more points. Conversely for larger C values, the model will chose a hyperplane with a smaller margin between the two classes; so therefore likely to make fewer misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the model and develop a simple grid search against some key parameters\n",
    "SVM_param_C=[0.001,0.01,0.1,1.0,10,100,1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep track of our best parameters\n",
    "SVM_best_score=0\n",
    "SVM_best_param=0\n",
    "\n",
    "# we will setup a manual grid search, but you can also use the gridsearchCV capability in sklearn\n",
    "for i in SVM_param_C:\n",
    "    svcclass=svm.SVC(C=i, kernel='rbf')\n",
    "    scores=cross_val_score(svcclass,\n",
    "                           features,\n",
    "                           target,\n",
    "                           scoring='accuracy')\n",
    "    if np.mean(scores)>SVM_best_score:\n",
    "        SVM_best_score=np.mean(scores)\n",
    "        SVM_best_param=i\n",
    "\n",
    "# print the overall best results\n",
    "print('Best Settings: C:',SVM_best_param)\n",
    "print('Score:',SVM_best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='best_svm'></a>\n",
    "## Understanding our Best SVM Model\n",
    "Like on all previous models, we now train our SVM on the all of the data to evaluate it further.\n",
    "\n",
    "Unfortunately SVMs are relatively opaque when it comes to interpreting the contributions of the different features in our model. However, to get an idea of how well it is fitted to our data, we can still calculate the in-sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep our best model (hyperparameters tuned)\n",
    "svcclass=svm.SVC(C=SVM_best_param, kernel='rbf')\n",
    "SVM=svcclass.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-sample error\n",
    "SVM.score(features,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this instance, we can see that the in-sample performance is significantly higher than the out-of-sample performance. As we have discussed previously, this is a bad sign and an indication that the dreaded overfitting has ocurred meaning the model will struggle to generalise well to unseen data. \n",
    "\n",
    "When models do not fit correctly, there are usually steps we can take to adapt the data to the model or adapt the model to the data. For the former, SVM's usually benefit significantly from a _normalisation_ of their input (transforming the features to _z-scores_ of themselves), so this would be a logical next step to improve performance.\n",
    "\n",
    "For the latter we could take a further look at the hyperparameters; we could try a finer gridsearch of the values closest to our current best value and also look at using a different type of _kernel_. The current kernel we are using is a non-linear one but using a non-linear kernel would make non-linear classification problems solvable (the original linear SVM can only solve linearly sepearable classes). However, this also increases the complexity of the model and therefore the potential for overfitting.\n",
    "\n",
    "Exploring these possibilities are beyond the scope of this tutorial but the reader is encouraged to explore themselves to see if they can fit a better SVM model. \n",
    "\n",
    "<a id='model_perf_comp'></a>\n",
    "# Model Performance Comparison\n",
    "\n",
    "Now that we have trained four models with our dataset, let's visualise their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the out-of-sample and in-sample performance of our models\n",
    "scores = pd.DataFrame({'Baseline':[sum(target)/len(target), sum(target)/len(target)],\n",
    "                       'Decision Tree':[DT_best_score, DT.score(features,target)], \n",
    "                       'Random Forest':[RF_best_score, RF.score(features,target)],\n",
    "                       'Logistic Regression':[LR_best_score, LR.score(features,target)], \n",
    "                       'SVM':[SVM_best_score, SVM.score(features,target)] },\n",
    "                      index=['Out-of-Sample','In-sample'])\n",
    "\n",
    "scores.plot(kind='bar', colormap='Blues')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to summarise our results we can see that all of our models with, the exception of the SVM, have similar performance for in-sample and out-of sample tests. This is a good indication that those models are well-fitted and have identified the latent structure that we are interested in.\n",
    "\n",
    "For our SVM classifier, as the out-of-sample performance is much lower than the in-sample performance, we can assume that this model is not well-fitted and, without, further tuning is not a good choice to use with new data.\n",
    "\n",
    "The decision-tree and the random forest models were essentially indistinguishable in their out-of-sample accuracy while the logistic regression model performance was marginally lower than them both. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this course we have used a group of statistical learning models called classifiers. \n",
    "- We discussed how training a model is equivalent to estimating parameters.  \n",
    "- We discussed the importance of cross-validation; splitting our dataset into training sets (for parameter estimation) and test sets (for model evaluation). This process reduces the likelihood of overfitting; the phenomenon whereby our model is tuned to the underlying idiosynchrasies of the our training data, rather than the latent structure we are trying to model. This model therefore fails to generalise when it sees new data.\n",
    "- We discussed how models can have different hyperparameters and how these can be estimated.\n",
    "- Finally, we used our data to train and introduce four specific classifier models. We walked through the process of hyperparameters selection and the significance of the difference between in-sample and out-of-sample performance.\n",
    "\n",
    "In the next course we will have a look at a different class of statistical learning models: regressors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
