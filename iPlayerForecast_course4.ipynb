{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Four: Build a Regressor\n",
    "\n",
    "Expected time to complete: 1 hour\n",
    "\n",
    "## Goal of this Course\n",
    "During this course we will build a regressor to forecast the minutes watched within the last two-week group.\n",
    "\n",
    "This course is split into the following parts:\n",
    "- <a href='#context'>Context</a> \n",
    "- <a href='#model_evaluation'>Model Evaluation</a> \n",
    "- <a href='#baseline_forecast'>Baseline Forecast</a>\n",
    "    - <a href='#load_data'>Load the Data</a>\n",
    "    - <a href='#define_baseline'>Define a Baseline</a>\n",
    "\n",
    "\n",
    "- <a href='#linear_reg'>Multivariate Linear Regression</a>\n",
    "    - <a href='#model_training'>Model Training</a>\n",
    "    - <a href='#linear_reg_und'>Understanding our Multivariate Linear Regression</a>\n",
    "\n",
    "\n",
    "- <a href='#ridge_reg'>Ridge Regression</a>\n",
    "    - <a href='#model_training2'>Model Training and Hyperparameters Tuning</a>\n",
    "    - <a href='#best_ridge_reg'>Understanding our Best Ridge Regression</a>\n",
    "\n",
    "\n",
    "- <a href='#lasso_reg'>Lasso Regression</a>\n",
    "    - <a href='#model_training3'>Model Training and Hyperparameters Tuning</a>\n",
    "    - <a href='#best_lasso_reg'>Understanding our Best Lasso Regression</a>\n",
    "\n",
    "\n",
    "- <a href='#model_perf_comp'>Model Performance Comparison</a> \n",
    "- <a href='#combine_mod'>Combine the Regression and Classification Models</a>\n",
    "\n",
    "<a id='context'></a>\n",
    "# Context\n",
    "In the previous tutorial we explained how the training process works and how to evaluate our models in the classification framework. We built four models that predicted whether a user would consume content within a two-week period based upon their past behaviour over the previous 14 weeks. \n",
    "\n",
    "Forecasting whether or not a given user will return is very useful; it provides valuable insight and feedback on the way we are surfacing content to our users and, in particular,  identifies the users where we need to do better.\n",
    "\n",
    "Data on the engagement of returning audience members is also useful. From the pool of audience members that did return, there will be valuable variation in the amount of content they consume. The aim of this current tutorial is to forecast the _quantity_ of content consumed by an audience member within a two-week period, based upon their consumption behaviour in the previous weeks. \n",
    "\n",
    "In order to forecast the minutes watched within the final two-week group we will first define our evaluation metric. As we did for the classifiers in the previous tutorial, we will then train different models, compare their performances to a baseline and benchmark their results. \n",
    "\n",
    "\n",
    "<a id='model_evaluation'></a>\n",
    "# Model Evaluation\n",
    "\n",
    "Evaluating the performance of a regressor is slightly less straightforward than a classifier although it is based upon the same principle: we define an error, then compare the error of our model to the expected error of a baseline model.   \n",
    "\n",
    "For regressors, the error is computed as the average residual between the actual observations and the model predictions. \n",
    "\n",
    "_Root mean squared error_ (RMSE) and _mean absolute error_ (MAE) are two commonly used examples of these. Once we have a representation of the error produced by the model we can compare that error with the error of the most simple model of the data (usually the mean). The statistic that reflects this difference is known as the coefficient of determination, or _R-squared_. R-squared measures the _goodness-of-fit_ of a model and can be thought of as the proportion of variance in the model output y that is captured by our model. In order to control for the complexity of the regressions we usually consider the _Adjusted R-squared_. We won't compute any of these statistics here but it's a good exercice for the reader.\n",
    "\n",
    "<a id='baseline_forecast'></a>\n",
    "# Baseline Forecast\n",
    "\n",
    "<a id='load_data'></a>\n",
    "## Load the Data\n",
    "First, let's get our data back and check to make sure it looks familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "# Comment below if you want to display the warnings (quite verbose here in the modeling part)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We put both target arrays (regression and classification) in the same txt file\n",
    "# As both target arrays have the same size we just need to split it it two\n",
    "# and get the correct part for the prediction task\n",
    "target = np.split(np.loadtxt('target.txt'), 2)[0].flatten()\n",
    "features = pd.read_csv('features.csv')\n",
    "\n",
    "# User id as index\n",
    "features = features.set_index('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we have the right input database\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we have the right output\n",
    "target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='define_baseline'></a>\n",
    "## Define a Baseline\n",
    "\n",
    "As mentionned in the previous course we should have a baseline to compare the performance of our models with. Here, as we are forecasting a quantitative variable, the most simple model is the one that predicts a constant value, such as  zero, the mean or the median of our data. \n",
    "\n",
    "Note that by default `scikit` _maximises_ an inverse error function as opposed to minimising an error function.  We therefore negate the error we compute for our baseline model to make it comparable with the performance metric we will pull out of our statistical models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a baseline to compare our results to (mean and median minutes watched and 0)\n",
    "mean=np.mean(target)\n",
    "median=np.median(target)\n",
    "\n",
    "mean_forecast=[mean]*len(target)\n",
    "median_forecast=[median]*len(target)\n",
    "zero_forecast=[0]*len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean: \"+str(mean))\n",
    "print(\"Median: \"+str(median))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median is null, meaning that more than half of our users did not consume content on iPlayer within the last two weeks.\n",
    "\n",
    "There is no need to consider the constant model equal to 0 as a baseline here (same as the median one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the errors for these different baselines\n",
    "from sklearn import metrics\n",
    "print(\"Score if we forecast the mean:\",\n",
    "      -metrics.mean_absolute_error(target,mean_forecast))\n",
    "print(\"Score if we forecast the median:\",\n",
    "      -metrics.mean_absolute_error(target,median_forecast))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We will now train a series of different kinds of regression models.\n",
    "\n",
    "These models should provide great business insight as we can _quantify the impact_ of each input variable upon our _engagement_ output variable.\n",
    "\n",
    "<a id='linear_reg'></a>\n",
    "# Multivariate Linear Regression\n",
    "\n",
    "The easiest way to relate our output variable to our input is to use a _linear combination_.\n",
    "\n",
    "A linear regression model decomposes the observed output in an additive way such that the coefficients we estimate in the training process can be interpreted as the impact of a given input all other things being equal (AOTBE). For more details see: https://en.wikipedia.org/wiki/Linear_regression\n",
    "\n",
    "And for the `scikit learn` documentation: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a simple regression\n",
    "from sklearn import linear_model\n",
    "\n",
    "# We will use cross validation, so import helper functions for this\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "# Plots\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model_training'></a>\n",
    "## Model Training\n",
    "\n",
    "For this model there is no hyperparameter to tune. Indeed, the model only has to evaluate the _contribution_ of each input variable to the linear combination.  \n",
    "\n",
    "Maintaining a train/test split is just as important when using regression models as it is with classification models, so we will use the `cross_val_score` function again here. This gives us the negative mean absolute error for each of the _k_ validation folds. In addition we use the `cross_val_predict` function to get the raw linear predictions. This function returns the predicted target value of each datapoint in our dataset while ensuring the model used to predict each datapoint has not been trained upon that datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We setup a simple linear regression, again using cross validation\n",
    "reg=linear_model.LinearRegression()\n",
    "\n",
    "MR_oos=cross_val_score(reg, features, target, scoring='neg_mean_absolute_error')\n",
    "MR_r2 = cross_val_score(reg, features, target, scoring='r2')\n",
    "predicted=cross_val_predict(reg, features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean error obtained in the CV\n",
    "print(\"Mean inverse out-of-sample error:\", np.mean(MR_oos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we now have an indication of the performance of our first regressor model. Fortunately, the score is higher than either of our baseline scores (-115 and -76) meaning that our model is capturing some meaningful information about our output _engagement_ variable.\n",
    "\n",
    "We can also plot our predicted values against the output we actually observe. The closer the dots to the first bisector, the better are our forecasts. \n",
    "\n",
    "We can also evaluate the strength of the linear relationship between the predicted and actual output values by calculating the correlation of the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's see how correlated the predicted and actual output variables are.\n",
    "from scipy.stats.stats import pearsonr\n",
    "MR_r, p  = pearsonr(target,predicted)\n",
    "print('Pearson correlation coefficient: {}'.format(MR_r) )\n",
    "print(\"P-value {}\".format(p))\n",
    "\n",
    "# Let's compare graphically the predicted and actual values\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(target, predicted)\n",
    "ax.plot([target.min(), target.max()], [target.min(), target.max()], 'k--', lw=1)\n",
    "ax.set_xlabel('Observed')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a correlation coefficient 0.8 represents a strong linear relationship between the two variables. The P-value of 0.0 represents the significance of the test. The __p-value__ is a number between 0 and 1 and can be interpreted as the probability that the reported effect is not real and actually happened by chance. In most behavioural studies p-values of below 0.05 are considered significant. Be aware that this significance level is, essentially, arbitrary and varies from field to field. \n",
    "\n",
    "From inspecting the scatter plot we see the illustration of the reported linear relationship. One thing to note from the plot is that an outlier exists in the top right corner. It is worth mentioning that a single outlier like that can have a significant impact upon correlation coefficient. So, for a more accurate reflection of the linear relationship, that outlier should be removed _before_ performing the correlation test, or even before the modeling part.\n",
    "\n",
    "\n",
    "<a id='linear_reg_und'></a>\n",
    "## Understanding our Multivariate Linear Regression\n",
    "\n",
    "Now that we have evaluated the performance of our model using the out-of-sample error, next, let's see if we can understand how the model is working. To do this, we will train one model using our entire dataset and the estimated coefficients will represent the importance the model is affording to each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train on all data \n",
    "lin_reg=linear_model.LinearRegression()\n",
    "MR=lin_reg.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-sample error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "MR_is = -metrics.mean_absolute_error(target,MR.predict(features))\n",
    "print(\"Inverse in-sample error: {}\".format(MR_is))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our inverse in-sample error is -56.6 while our inverse out-of-sample (average) error is -58. This looks sensible as, remember that, in-sample accuracy will always be higher than out-of-sample one. The fact that these two values are not far apart is an indication that the model is well-fitted.\n",
    "\n",
    "Now let's have a look at the estimated coefficients to understand the drivers of the _volume_ of minutes watched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's have a look at the highest magnitude positive coefficients.\n",
    "coefMR=pd.DataFrame(\n",
    "    {'feature': list(features.columns),\n",
    "     'coefficient': list(MR.coef_)\n",
    "    })\n",
    "coefMR.sort_values(by='coefficient', ascending=False).reset_index(drop=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets have a look at the highest magnitude negative coefficients.\n",
    "coefMR.sort_values(by='coefficient', ascending=False).reset_index(drop=True).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now let's plot them all\n",
    "coefMR = coefMR.set_index(['feature'])\n",
    "coefMR.sort_values(by='coefficient', ascending=False).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are quite interesting here. It seems that some particular habits __drive__ the amount of content consumed.\n",
    "\n",
    "The table of positive coefficients indicate the features that are positively correlated with minutes watched. So, users that consume most iPlayer content on Wednesdays, or in the afternoon, or whose favourite genre is Sport are more likely to consume more content overall. Conversely, users whose favourite genre is Children's or who consume most of their content on a Monday do not consume - in terms of total of minutes - as much. It's difficult to speak of drivers for engagement here because the minutes watched is highly correlated to the type of content. For example, Children's  content is, on average, shorter than Sport or Drama so it makes sense that people that watch long shows watch more minutes than people that watch short shows. It doesn't necessarily mean than the user's who consume only Children's content are less engaged. \n",
    "\n",
    "More specifically, the estimated coefficients can be interpreted as follows. Let's take the `num_genre` variable, whose estimated coefficient and therefore model contribution is 1.3. This means that, if we hold all other features constant, for each user that views one additional genre in the 14-week training period (`num_genre+=1`), the model output prediction will get additional 1.3 minutes of consumed content. While this feature alone would not make a very accurate model for our purpose, it makes sense that the more genres an audience member consumes, the more engaged they are.\n",
    "\n",
    "NB: For the estimated coefficients we should in theory have a look at the _p-values_ for each; statistic values that indicate whether our estimated coefficient is significantly different from zero. The reader is encouraged to explore this topic as an exercise.\n",
    "\n",
    "<a id='ridge_reg'></a>\n",
    "# Ridge Regression\n",
    "In many situations, trained multivariate linear regression models have very large coefficients. Unfortunately, experience suggests that large coefficients tend to lead to more complex models that overfit the data. _Regularization_ models various techniques to avoid this problem.\n",
    "\n",
    "Ridge regression, also referred to as _L2 regularization_ is a variant of multivariate regression that adds a term, _alpha_, that penalises the sum-squared of the coefficients. In practical terms, an alpha of zero will train a model that is identical to multivariate linear regression and as we increase _alpha_ the magnitude of the coefficients will be increasingly constrained.\n",
    "\n",
    "`Scikit documentation`: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "<a id='model_training2'></a>\n",
    "## Model Training and Hyperparameters Tuning\n",
    "The _alpha_ is a hyperparameter that we need to tune.  Let's use the same approach we used for the classification models and do a grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model and develop a simple grid search against some key parameters\n",
    "RR_param_alpha=[0.001,0.01,0.1,1.0,10,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep track of our best parameters\n",
    "RR_best_score=-200\n",
    "RR_best_param=0\n",
    "\n",
    "# we will setup a manual grid search, but you can also use the gridsearchCV capability in sklearn\n",
    "for i in RR_param_alpha:\n",
    "    reg_r = linear_model.Ridge(alpha = i)\n",
    "    scores=cross_val_score(reg_r,\n",
    "                           features,\n",
    "                           target,\n",
    "                           scoring='neg_mean_absolute_error')\n",
    "    \n",
    "    if np.mean(scores)>RR_best_score:\n",
    "        RR_best_score=np.mean(scores)\n",
    "        RR_best_param=i\n",
    "\n",
    "# print the overall best results\n",
    "print('Best Settings: alpha:',RR_best_param)\n",
    "print('Inverse out-of-sample error:',RR_best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a ridge regression model does _slightly_ better than our previous multivariate linear regression model.\n",
    "\n",
    "Now let's look at the relationship between the predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First let's see how correlated the predicted and actual output variables are.\n",
    "reg_r=linear_model.Ridge(alpha=RR_best_param)\n",
    "predicted=cross_val_predict(reg_r,features,target)\n",
    "RR_r, p  = pearsonr(target,predicted)\n",
    "print('Pearson correlation coefficient: {}'.format(RR_r) )\n",
    "print(\"P-value {}\".format(p))\n",
    "\n",
    "# Let's compare graphically the predicted and actual values\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(target, predicted)\n",
    "ax.plot([target.min(), target.max()], [target.min(), target.max()], 'k--', lw=1)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data (both the plot and the correlation coefficient) look very similar to the data produced by the previous linear regression model so there are no more insights to be made here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='best_ridge_reg'></a>\n",
    "## Understanding our Best Ridge Regression\n",
    "\n",
    "As before, to explore our best ridge regression model we will train a new model using the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep our best model (hyperparameters tuned)\n",
    "ridge=linear_model.Ridge(alpha=RR_best_param)\n",
    "RR=ridge.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-sample error\n",
    "RR_is=-metrics.mean_absolute_error(target,RR.predict(features))\n",
    "print(\"Inverse in-sample error: {}\".format(RR_is))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The in-sample performance metric looks sensible.\n",
    "\n",
    "Now let's look at the coefficients where we should see a difference from our linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the estimated coefficients\n",
    "coefRR=pd.DataFrame(\n",
    "    {'feature': list(features.columns),\n",
    "     'coefficient': list(RR.coef_)\n",
    "    })\n",
    "# Make the index the feature label.\n",
    "coefRR = coefRR.set_index(['feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's have a look at the highest magnitude positive coefficients.\n",
    "coefRR.sort_values(by='coefficient', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, let's have a look at the highest magnitude negative coefficients.\n",
    "coefRR.sort_values(by='coefficient', ascending=False).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot them all\n",
    "coefRR.sort_values(by='coefficient', ascending=False).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the shape of the distribution of coefficients has not changed noticebly, however, the absolute values of those coefficients is much lower than it was for the linear regression model.\n",
    "\n",
    "<a id='lasso_reg'></a>\n",
    "# Lasso Regression\n",
    "\n",
    "Lasso regression is another variant of multivariate regression and is often referred to as _L1 regularisation_. Instead of penalising the sum-squared of the coefficients (as in ridge regression), lasso regression penalises the absolute sum square of the coefficients. \n",
    "\n",
    "In practical terms, this has the effect of limiting the number of non-zero coefficients in the model. This makes lasso regression a convenient method of feature selection.\n",
    "\n",
    "`Scikit documentation`: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "\n",
    "<a id='model_training3'></a>\n",
    "## Model Training and Hyperparameters Tuning\n",
    "\n",
    "Again, we need to tune the penalty parameter _alpha_. In lasso regression, the _alpha_ parameter varies the number of non-zero coefficients. An _alpha_ of zero will produce a model that is equal to linear regression and as _alpha_ increases the number of non-zero coefficients is increasingly constrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model and develop a simple grid search against some key parameters\n",
    "LR_param_alpha=[0.001,0.01,0.1,1.0,10,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep track of our best parameters\n",
    "LR_best_score=-200\n",
    "LR_best_param=0\n",
    "\n",
    "# we will setup a manual grid search, but you can also use the gridsearchCV capability in sklearn\n",
    "for i in LR_param_alpha:\n",
    "    reg_l = linear_model.LassoLars(alpha = i)\n",
    "    scores=cross_val_score(reg_l,\n",
    "                           features,\n",
    "                           target,\n",
    "                           scoring='neg_mean_absolute_error')\n",
    "    if np.mean(scores)>LR_best_score:\n",
    "        LR_best_score=np.mean(scores)\n",
    "        LR_best_param=i\n",
    "\n",
    "# print the overall best results\n",
    "print('Best Settings: alpha:',LR_best_param)\n",
    "print('Score:',LR_best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our best alpha parameter is 0.1 and our best performance metric is -56, which is marginally better than both of our previous models (achieving -58 and 57.6).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's see how correlated the predicted and actual output variables are.\n",
    "reg_l=linear_model.LassoLars(alpha=LR_best_param)\n",
    "predicted=cross_val_predict(reg_l,features,target)\n",
    "LR_r, p  = pearsonr(target,predicted)\n",
    "print('Pearson correlation coefficient: {}'.format(LR_r) )\n",
    "print(\"P-value {}\".format(p))\n",
    "\n",
    "# Let's compare graphically the predicted and actual values\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(target,predicted)\n",
    "ax.plot([target.min(), target.max()], [target.min(), target.max()], 'k--', lw=1)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the scatter plot and correlation statistics look very similar to those generated using data from our previous regression models. So there are no further insights to be made here.\n",
    "\n",
    "<a id='best_lasso_reg'></a>\n",
    "## Understanding our Best Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep our best model (hyperparameters tuned)\n",
    "lasso=linear_model.LassoLars(alpha=LR_best_param)\n",
    "LR=lasso.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-sample error\n",
    "LR_is=-metrics.mean_absolute_error(target,LR.predict(features))\n",
    "print(LR_is)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our in-sample error looks sensible. Now let's look at the estimated coefficients.\n",
    "\n",
    "We should expect to see quite significant differences in the distribution of coefficients in our lasso model due to how it penalises the number of non-zero weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the estimated coefficients\n",
    "coefLR=pd.DataFrame(\n",
    "    {'feature': list(features.columns),\n",
    "     'coefficient': list(LR.coef_)\n",
    "    })\n",
    "# Make the index the feature label.\n",
    "coefLR=coefLR.set_index(['feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's have a look at the highest magnitude positive coefficients.\n",
    "coefLR.sort_values(by='coefficient', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's have a look at the highest magnitude positive coefficients.\n",
    "coefLR.sort_values(by='coefficient', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot them all\n",
    "coefLR.sort_values(by='coefficient', ascending=False).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as expected we see a drastically different distribution of coefficients to our previous two models, with only seven of the original thirty-four coefficients included different from zero.\n",
    "\n",
    "Interestingly, the lasso model has also chosen very different features to the ones weighted highest by the previous two models. The previous two models achieve their highest accuracy from quite a complex combination of positive and negative coefficients that describe the most frequent genre of content and viewing time; while they weighted minutes watched in previous weeks relatively low.\n",
    "\n",
    "Conversely, the lasso model improves on those previous models by discarding all the genre and viewing time features and including just seven positive coefficients that weight the minutes watched in previous weeks. \n",
    "\n",
    "<a id='model_perf_comp'></a>\n",
    "# Model Performance Comparison\n",
    "\n",
    "Now that we have trained tree different models with our dataset, let's visualise their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will negate the accuracy here as it's easier to read \n",
    "# and then plot the errors: the best model is the one than MINIMIZES these values\n",
    "scores = pd.DataFrame({'Baseline (median)':[metrics.mean_absolute_error(target,median_forecast),\n",
    "                                            metrics.mean_absolute_error(target,median_forecast)],\n",
    "                       'Linear Regression':[-np.mean(MR_oos), -MR_is], \n",
    "                       'Ridge Regression':[-RR_best_score, -RR_is],\n",
    "                       'Lasso Regression':[-LR_best_score, -LR_is]},\n",
    "                      index=['Out-of-Sample','In-sample'])\n",
    "\n",
    "scores.plot(kind='bar', colormap='Blues')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this plot that the performance of our regression models are difficult to tell apart. The original multivariate regression model and the ridge regression are particularly difficult to distinguish while the lasso model exhibits marginally the best performance, in terms of MAE.\n",
    "\n",
    "On the basis of these results, it is likely that each of the three models would not perform significantly better than each other on new data but if we follow _Occam's razor_, the lasso model makes the fewest assumptions, is the least complex, and is therefore most likely to generalise to new data.\n",
    "\n",
    "<a id='combine'></a>\n",
    "# Combine the Regression and Classification Models\n",
    "Now that we have a model to predict whether a user will consume on the last two-week group, and a model that forecast the minutes watched, we can combine the two in order to generate an overall score.\n",
    "\n",
    "For the classifier we will use a Random Forest with the following hyperparameters: Max Depth 6, Min Sample Leaf 75. For the regression we will use the Lasso framework with a regularisation parameter equal to 0.1.\n",
    "\n",
    "<a id='retrain_classif'></a>\n",
    "## Retrain our Classifier\n",
    "Let's first get our data back for the classification output and then retrain our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We put both target arrays (regression and classification) in the same txt file\n",
    "# As both target arrays have the same size we just need to split it it two\n",
    "# and get the correct part for the prediction task\n",
    "target_class = np.split(np.loadtxt('target.txt'), 2)[1].flatten()\n",
    "target_class[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier with the hyperparameters tuned from last course\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forrestclass=RandomForestClassifier(n_estimators=200,\n",
    "                                    max_depth=6,\n",
    "                                    min_samples_leaf=75)\n",
    "RF=forrestclass.fit(features,target_class)\n",
    "print(\"In-sample accuracy RF: \"+str(RF.score(features,target_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has a good accuracy. Let's have a look at the other metrics that we usually use to assess the performance of a classifier: __precision__ and __recall__.\n",
    "\n",
    "For more details see: https://en.wikipedia.org/wiki/Precision_and_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the confusion matrix of our classifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "class_names=[0,1]\n",
    "\n",
    "# We found this code here:\n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print('Normalized confusion matrix')\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix=confusion_matrix(target_class, RF.predict(features))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _confusion matrix_ help us to understand the performance of our classifier. Overall, our accuracy is quite good - looking at the number of correct predictions (diagonal of the matrix).\n",
    "\n",
    "Let's have a look at the other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall: how many relevant items (class 1) are selected?\n",
    "# Or in other words, what is the proportion of class 1 our model managed to get?\n",
    "nb_class1=cnf_matrix[1][0]+cnf_matrix[1][1]\n",
    "nb_class1_pred_true=cnf_matrix[1][1]\n",
    "print('Among the '+str(nb_class1)+\n",
    "      ' users who consumed content, the classifier identified '+str(nb_class1_pred_true)+\n",
    "      '. \\nRecall: '+str(nb_class1_pred_true/nb_class1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision: how many selected items (predicted as class 1) are relevant?\n",
    "# Or in other words, what is the proportion of true class 1 in our class 1 predictions?\n",
    "nb_class1_pred_false=cnf_matrix[0][1]\n",
    "print('Among the '+str(nb_class1_pred_false+nb_class1_pred_true)+\n",
    "      ' users our model forecasted to consume content, '+str(nb_class1_pred_true)+\n",
    "      ' did. \\nPrecision: '+\n",
    "      str(nb_class1_pred_true/(nb_class1_pred_true+nb_class1_pred_false)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='overall_score'></a>\n",
    "## Compute the Overall Score and it's Performance\n",
    "Let's now combine the outputs of our classifier and our regressor to forecast the minutes watched.\n",
    "\n",
    "To do so we will simply multiply our two outputs:\n",
    "- If the classifier forecast for a given user class 1 (i.e. the user consumes content) then the overall score output will be the one of the regressor.\n",
    "- If the classifier forecast class 0 (i.e. he or she doesn't consume content) then the overall score will be zero, and thus even if our regressor forecasts a number of minutes watched non equal to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute our overall score by multiplying the two outputs\n",
    "# Note that we can easily do that are we already controlled for the order in our users\n",
    "overall_score=RF.predict(features)*LR.predict(features)\n",
    "overall_score[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now have a look at the in-sample error\n",
    "comb_is=-metrics.mean_absolute_error(target,overall_score)\n",
    "print(comb_is)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination of the two models improves the overall performance of our regression task.\n",
    "\n",
    "# Summary\n",
    "\n",
    "In this tutorial we have explored how to train statistical models that can handle linear output variables; regressors.\n",
    "\n",
    "- We looked at the different ways in which the performance of a regression model can be evaluated and the importance of establishing a baseline of error.\n",
    "- We used our data to train three different types of regression models and evaluated their performance: multivariate linear regression, ridge regression and lasso regression.\n",
    "- We learnt about the concept of _regularization_; an approach that aims to reduce the problem of very large coefficients in regression models which can lead to overfitting.\n",
    "- We learnt how to interpret a regression model by inspecting the coeffcients. Here we found clear differences between the three models, giving us a more intuitive understanding of how the workings of the models differed.\n",
    "\n",
    "# Training Recap\n",
    "\n",
    "Over the four tutorials in this course we have covered a lot. You may not feel like an expert data science yet but if you're starting to feel comfortable with everything we have done so far you are certainly on your way there. \n",
    "\n",
    "To recap, the first tutorial helped us understand how to _formulate our data science problem_ and select the dataset we need to solve it. We decided that it would be useful to try and predict engagement statistics of users within a two-week period, based upon their viewing behaviour in the previous 14 weeks. To solve this problem we decided to use a dataset containing the views of 10,000 iPlayer viewers.\n",
    "\n",
    "In the second tutorial we introduced the concept of _data wrangling_; transforming and preparing our dataset so that it is ready to be ingested into our statistical learning models. It is these models that allow us to make engagement predictions and forecasts about our users.\n",
    "\n",
    "In the third tutorial, we were introduced to our first _classifier models_; how to train them and how to evaluate their performance. We used four classifiers to forecast whether or not a user was _engaged_ with iPlayer, in a sense that he or she will consume content on the last two-week group.\n",
    "\n",
    "In this final tutorial, we introduced our first _regression models_. These allowed us to forecast the amount of engagement with iPlayer content. Finally, we combine the two type of modeling to get the most of our data and the patterns found in it. \n",
    "\n",
    "We hope you enjoyed joining us on this course and that you found it helpful in your development as a world-leading data scientist!\n",
    "\n",
    "-Datalab Team"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
